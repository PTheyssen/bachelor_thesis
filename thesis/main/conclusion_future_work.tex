% !TeX spellcheck = en_US
% !TeX encoding = UTF-8
% !TeX root = ../thesis.tex

\chapter{Conclusion and Future Work}


\section{Conclusion}
In this work, we have presented an approach for combining the MORE algorithm
\citep{abdolmaleki2015model} with recursive surrogate-modeling
using the RLS algorithm with drift model (\Cref{RLS:basic}).
The main goal of this approach is to improve the sample efficiency of
MORE, as real world samples are expensive and
previously learning the surrogate model has been data-intensive.
We tested our algorithm on the Rosenbrock function and
a two simulated robotic task.
While we were able to clearly improve sample efficiency on the Rosenbrock
function, we did not manage to achieve similar
improvements on the robotic tasks.
Nevertheless we could successfully learn the planar reaching task and
the ball-in-a-cup task,
which shows that the recursive surrogate-modeling approach has
potential to reach and improve the current state-of-the art in terms
of sample efficiency.
Our approach leaves much room for improvement in terms of finding
the optimal combination of hyperparameters and different data
preprocessing techniques.
With more research the improved sample efficiency observed
on the optimization test function may be transferred to
complex robotic tasks.

\section{Future Work}
This section lists possibilities for further improvements of using
recursive surrogate modeling for the MORE algorithm.

\textbf{Using a State Transition Model.}
As a next step finding a fitting state transition matrix $\mathbf{A}$ and
using the prediction step of the Kalman filter may improve results. 
The author had little experience with Kalman filters prior
to this project, hence work on optimizing the algorithm for
recursive estimation may lead to significant improvements.

\textbf{Re-evaluating, optimizing hyperparameters.}
One disadvantage of MORE is the number of parameters, our
recursive surrogate-modeling algorithm introduced additional parameters.
As we relied mainly on manual and grid search for hyperparameters a
more sophisticated approach could yield considerable improvements.
Though the stochastic nature of the algorithm makes comparing
different configurations difficult and a more thorough evaluation
could reveal new insights.

\textbf{Learning the Model Noise.}
When using no pool the model noise
parameter stays constant for all MORE
iterations. Although we optimized this constant value it makes more sense to
dynamically adjust the model
noise in some way related to the changes values of the objective function.
In the future exploring different techniques for learning the model drift
may proof successful in increasing the performance.

\textbf{Testing on Complex Robot Learning Tasks}
We only conducted experiments on a simple simulated robot
task. The obvious next step is to benchmark the algorithm on more
complicated dimensional robotic tasks.
This might reveal shortcomings of the approach
for dealing with unsmooth objective functions and fulfilling the goal
of safe exploration.
% Additionally for higher dimensional tasks it may be necessary to
% introduce some form of dimensionality reduction as for our
% 5-link reaching task the RLS already has to estimate
% 351 parameters.
