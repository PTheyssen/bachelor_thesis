% !TeX spellcheck = en_US
% !TeX encoding = UTF-8
% !TeX root = ../thesis.tex

\chapter{Recursive Surrogate-Modeling for MORE}
In this chapter, we will first discuss the quadratic surrogate model
and motivate the idea of recursively
estimating it.
Then we formulate the surrogate model estimation task
as a regression problem.
Next we introduce a batch solution
that is based on ridge regression and
the Recursive Least Squares with Drift Model algorithm
as our recursive approach for solving the problem.
% Finally we describe how to connect our approaches
% with the MORE algorithm.

\section{Quadratic Surrogate Model}
\label{sec:surrogate}
To compute the policy update the MORE algorithm
uses a quadratic surrogate model of the objective function.
The surrogate model has the following form
\begin{align*}
  % TODO: explain why using 1/2 or not?
  f(\mathbf{x}) \approx \hat{f}(\mathbf{x}) =
  %-\frac{1}{2}
  \mathbf{x}^T \mathbf{R} \mathbf{x}
  + \mathbf{x}^T \mathbf{r} + r
\end{align*}
where $f(\mathbf{x}) : \mathbb{R}^n \rightarrow \mathbb{R}$
is the original objective function.
The surrogate model $\hat{f}(\mathbf{x})$
has a quadratic term $\mathbf{R} \in \mathbb{R}^{n \times n}$,
a linear term $\mathbf{r} \in \mathbb{R}^{n}$ and a scalar $r$.
Since we assume that the search distribution is Gaussian,
we need to use a quadratic model in order to derive the closed
form solution for the update of the search distribution.
Using a Gaussian search distribution combined with a
quadratic model is sufficient as the exponential of the
Gaussian is also quadratic in the parameters and
a more complex model could not be exploited by
a Gaussian distribution \citep{abdolmaleki2015model}.
The original approach for estimating the surrogate model
employs a form of Bayesian dimensionality
reduction combined with linear regression.
This is done from scratch in each
iteration using samples and corresponding
values of the objective function. 
This approach turns out to be very data-intensive and computationally
expensive.
By employing the data preprocessing techniques introduced in
\Cref{sec:data_pre} we can instead
solve the problem in the original dimension.
Additionally, subsequent models are correlated
due to the locality of the data, see \Cref{fig:sur_model} for an example of
two subsequent surrogate models.
\begin{figure}[t]
  \centering
  \subfigure[][Objective Function]{\input{figures/rosenbrock_mean}}
  \subfigure[][Surrogate Model (iteration $i$)]{\input{figures/model_1}}
  \subfigure[][Surrogate Model (iteration $i+1$)]{\input{figures/model_2}}
  \caption{\small
    In (a) the original objective function,
    a 2 dimensional Rosenbrock function (\Cref{sec:test_func}) is shown.
    In (b) and (c) we see the search distribution mean,
    the surrogate model and the samples used to estimate
    the model from two subsequent MORE iterations.}
 \label{fig:sur_model}
\end{figure}
This arises from the fact that the KL-divergence is
used to bound the distance between
subsequent search distributions. Therefore recursive estimation
techniques may be able to utilize the information contained in the previous
surrogate model and thus reduce the total amount of samples needed.
This also may reduce the runtime of the algorithm.
Nevertheless estimating the surrogate model is a challenging task as we
have to estimate $\mathcal{O}(n^2)$ many parameters while using a minimal
amount of samples in each MORE Iteration. 


\section{Ridge Regression}
\label{sec:ridge}
Let us now introduce a batch solution for learning the model,
which we will use as a comparison
to the recursive approach in our experiments.
Therefore we formulate the task of estimating the surrogate model as
a regression problem (\ref{eq:regression}). For this we use
a feature function $\phi(\mathbf{x})$ which returns
a bias term, all linear and all quadratic terms. The dimensionality of
$\phi(\mathbf{x})$ is $D = 1 + d + d(d + 1) / 2$, where $d$
is the dimensionality of the parameter space.
\begin{align}
  \label{eq:regression}
  y = \phi(\mathbf{x}) \beta + \epsilon
\end{align}
% The surrogate model parameters are in $\beta$
% a vector of length $D$ containing first the
% scaler than $d$ linear terms and then $d(d+1) /2$ quadratic terms
% (the lower triangular matrix).
Our data consists of the samples and corresponding objective
values $\mathcal{D} = \{(\mathbf{x}_1, y_1),\dots,(\mathbf{x}_n, y_n)\}$.
To solve the regression problem we set up
the design matrix $\mathbf{X}$ as depicted in \cref{eq:lgs}.
A row of $\mathbf{X}$ contains a 1 as the first entry for the bias term.
Then the next $d$ entries are made up of the corresponding
sample $\mathbf{x}_k$ for the linear term.
The final $d(d + 1) / 2$  entries are
the lower triangular matrix of the product $\mathbf{x}_k \mathbf{x}_k^T$,
which we denote by tril$(\mathbf{x}_k \mathbf{x}_k^T)$.
% Corresponding to the set
% $\text{tri}(\mathbf{x}) = \{x_1^2 , x_1 x_2 , x_1 x_3 ,
% \cdots , x_2^2 , x_2 x_3 \cdots\}$,
% where we multiply each sample entry with itself and all entries with
% higher index. 

For the batch solution with $n \geq D$ samples
we get the following system of linear equations

\begin{equation}
  \label{eq:lgs}
\begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{bmatrix}
=
\underbrace{
\begin{bmatrix}
  1 & \mathbf{x}_1 & \text{tril}(\mathbf{x}_1 \mathbf{x}_1^T) \\
  1 & \mathbf{x}_2 & \text{tril}(\mathbf{x}_2 \mathbf{x}_2^T) \\
  \vdots & \vdots & \vdots \\
  1 & \mathbf{x}_n & \text{tril}(\mathbf{x}_n \mathbf{x}_n^T) \\
\end{bmatrix}}_{\mathbf{X}}
\begin{bmatrix}
  \beta_1 \\ \beta_2 \\ \vdots \\ \beta_D
\end{bmatrix}
+
\begin{bmatrix}
  \epsilon_1 \\
  \epsilon_2 \\
  \vdots \\
  \epsilon_n
\end{bmatrix}.
\end{equation}


We assume that the measurement noise $\epsilon_k$ is zero mean Gaussian
$\epsilon_k \sim N(0, \sigma^2)$ distributed.
To solve the system of linear equations in \ref{eq:lgs} we use
a form of ridge regression \citep{hoerl1975ridge}.
$$ \beta = (\mathbf{X}^T \mathbf{X} +
\lambda \mathbf{I})^{-1} \mathbf{X}^T \mathbf{y} $$
where $\lambda$ is the ridge parameter chosen as $1 \times 10^{-12}$.
The solution vector $\beta$ contains the surrogate model parameters
$(r, \mathbf{r}, \mathbf{R})$. As the regression is done
in the original problem space, which tends to be high dimensional especially
for robotic tasks, we now explore how data preprocessing techniques
can make the estimation task easier.

\begin{figure}[t]
  \centering
  \subfigure[][Without Whitening]{\input{figures/LS_no_whitening}}
  \subfigure[][In Whitened Space]{\input{figures/LS_whitening}}
  \subfigure[][Unwhitened Parameters]{\input{figures/LS_unwhitened}}
  \caption{\small
    Example of using MORE with a least squares approach for
    surrogate-modeling with and without whitening on the 2-dimensional
    Rosenbrock function (\Cref{sec:test_func}).
    The plots show the predicted surrogate model
    parameters $\mathbf{\beta}$. In (a) the parameters are estimated
    without whitening the samples. In (b) a whitening transformation is
    applied to the samples before parameter estimation
    and (c) shows the parameters from (b) after reversing
    the whitening transformation.
    We can see that the parameters in whitened space
    stay in a smaller range 
    making the estimation task easier.}
 \label{fig:whitening}
\end{figure}

For the samples we use \textit{Cholesky whitening} which is based on
Cholesky factorization of the precision matrix
$\mathbf{L}\mathbf{L}^T = \Sigma^{-1}$, which leads to the whitening
matrix $\mathbf{W}^{\text{Chol}} = \mathbf{L}^T$.
If the Cholesky whitening is unsuccessful due to numerical problems,
we instead standardize
the random vector meaning $\text{var}(\mathbf{z}) = 1$ but the correlations
are not removed.
In \Cref{fig:whitening} we provide an example which illustrates the effect of 
whitening for our parameter estimation task.

To reduce the number of samples needed per iteration we use a sample
pool, which contains samples from previous iterations.
To deal with data from unsmooth reward functions
caused by penalties (e.g. self-collision of the robot) we
normalize the reward.
For this the mean and standard deviation of the sample pool is used to compute
the z-score for each objective value $y$ as follows
\begin{equation*}
  \label{eq:norm}
  z = \frac{y - \mu_{\text{pool}}}{\sigma_{\text{pool}}}.
\end{equation*}
In our robotic tasks the reward signal can have sharp jumps due to
penalties (e.g. self-collision of the robot).
To avoid getting stuck after receiving a penalty we additionally
experimented with clipping the rewards.

% - talk about connection to batch solution from fundamentals
% the prior is not used (stays the same)
\section{Recursive Least Squares with Drift Model}
\label{sec:rls}
% We call our approach recursive least squares with drift model
% for the parameters (\Cref{RLS:basic}).
Let us now explain how we use the recursive least squares (RLS) with
drift model algorithm 
to compute the solution vector $\beta$ to the regression problem.
Using a recursive estimation approach we
process each pair of samples and rewards $(\mathbf{x}_k, y_k)$ one at
a time
$$
 y_k =
 \big(1 \;  \mathbf{x}_k \; \text{tril}(\mathbf{x}_k \mathbf{x}_k^T) \big)
\begin{pmatrix}
  \beta_1 \\ \beta_2 \\ \vdots \\ \beta_D
\end{pmatrix} 
+ \epsilon_k.
$$
As the model parameters do not stay constant between MORE iterations
we assume they perform
a Gaussian random walk between measurements and for an arbitrary
step $k$ we get
\begin{align*}
  p(y_k | \, \mathbf{\beta}_k) &= \mathcal{N}(y_k | \, \mathbf{H}_k \mathbf{\beta}_k, \sigma^2) \\
  p(\mathbf{\beta}_k | \, \mathbf{\beta}_{k-1}) &= \mathcal{N}(\mathbf{\beta}_k | \,
                               \mathbf{\beta}_{k-1}, \mathbf{Q}) \\
  p(\mathbf{\beta}_0) &= \mathcal{N}(\mathbf{\beta}_0 | \, \mathbf{m}_0, \mathbf{P}_0)
\end{align*}
where $\mathbf{Q}$ is the covariance of the random walk.
The parameter noise is added to the covariance matrix of the parameters
before each update step.
Our prior $\mathbf{m}_0$ is chosen such that for the
first surrogate model parameters
we set the quadratic term to an identity matrix $\mathbf{R} = \mathbf{I}$
and the other terms to zero $ \mathbf{r} = \mathbf{0}$ and $ r = 0$.
The covariance matrix is initialized as
$\mathbf{P}_0 = \delta \mathbf{I}$ with
the parameter $\delta$ controlling how confident
we are in our prior.
We want to compute the filtering distribution
\begin{align*}
  p(\beta_k | \, y_{1:k}, \mathbf{x}_{1:k})
  &\propto p(y_k, \mathbf{x}_k | \, \beta_k) \,
    p(\beta_k |\, y_{1:k-1}, \mathbf{x}_{1:k-1}) \\
  &\propto \mathcal{N}(\beta_k | \, \mathbf{m}_k, \mathbf{P}_k), 
\end{align*}
for this we can use the update equation
of the Kalman filter (\ref{KF_update}). 
Using the model noise to increase the covariance matrix of the parameters
combined with
the Kalman filter update step
forms the basis of \Cref{RLS:basic}.

We further explored adding various data preprocessing techniques
to the algorithm.
First of all, like the ridge regression we also use whitening on the samples.
To cope with sharp jumps in the objective function due to penalties
we normalize the reward $y_k$.
For computing the mean and standard deviation
we use the exponential moving average (EMA) (\Cref{eq:ema}),
thus for the standard score of the reward we get 
\begin{equation*}
  \label{eq:norm}
  z = \frac{y - \mu_{\text{EMA}}}{\sigma_{\text{EMA}}}.
\end{equation*}
By using the EMA the normalization can be computed iteratively
in an on-line fashion. In addition we can use the
weighting factor $\alpha$ of the EMA to control how fast
older rewards should be discounted.
% which computes the model parameters in a recursive fashion.
% To make the numerically more stable we also
% employ whitening for our recursive approach.

Opposed to the ridge regression we do not want to use a sample pool
for the RLS and instead rely on the information contained in
the previous surrogate model.
% In theory when using a recursive estimation approach
% the information in a sample pool is redundant and should be avoided.
Nonetheless in our experiments we did not receive good results when
using only new samples without a sample pool.
The predictions for the surrogate models were to inaccurate.
To overcome this we tried using a warm start, meaning we
process a large batch of samples in the first iteration to get 
an accurate prediction. The prediction can then be subsequently
updated with new samples. This improved the results but
it still performed worse than the batch solution.
% For our reaching task this produced worth investigating
% and then learning the model noise
% On optimization test functions like the Rosenbrock
% function \cref{sec:test_func} we did not receive
% promising results using only a warm start.
Thus we also tried using a sample pool for our recursive estimation approach.
% From a theoretical standpoint this is rather imprecise,
% but using it improved our result.
When using a sample pool we generally process the newest samples
last. Additionally, we explored giving older samples 
a greater model noise, encoding our increased uncertainty about them.
For this we simply introduced a counter for each sample,
indicating how many times the sample has been used.
For older samples (higher counter) we increase the model noise controlled
by a weight factor $\gamma$.
When using this sample weighting we increase the model noise by computing
$$ \mathbf{Q} + \gamma \,\cdot \text{counter} \, \cdot \mathbf{I} $$
before adding the model noise to the covariance
matrix of the parameters.
% We could improve our results on the Rosenbrock test function compared
% to the RLS version with constant model noise for all samples.
% cite comparison in evaluation chapter
Still using a sample pool is theoretically imprecise and part
of future work is to explore ways to avoid it.
One way to achieve this might be learning the model noise
or dynamically adjusting it to the changes in the objective values.
\begin{algorithm}[h]
\renewcommand{\algorithmcfname}{Algorithm}
\SetKw{KwInit}{Initialization:}
\DontPrintSemicolon
\SetAlgoLined
\KwIn{% $\{(\mathbf{x}_1,y_1),\cdots,(\mathbf{x}_N,y_N)\}$
  stream of samples as rows of design matrix $\mathbf{X}_n$
  and rewards $y_n$ with $n = 1, \dots, N$, \\
  $\mathbf{Q}$ model noise, $\sigma^2$ measurement noise}
\KwInit{$\mathbf{m}_0 = (0, \mathbf{0}, \mathbf{I})$,  $\mathbf{P}_0 = \delta \mathbf{I}$}

\For{$n = 1,...,N$}
{
  \Begin(Data Preprocessing)
  {
  Use whitening on $\mathbf{X}_n$\;
  Compute exponential moving average and normalize $y_n$ \;
  }
  % $~$ \;
  $\mathbf{P}_n^- = \mathbf{P}_{n-1} + \mathbf{Q}$
  \textit{  // Add the model noise}\;
  \Begin(Update step)
  {
    $S_n = \mathbf{X}_n \; \mathbf{P}_n^- \, \mathbf{X}_n^T + \sigma^2$ \;
    $\textbf{K}_n = \textbf{P}_{n}^{-} \, \textbf{X}^T_n \, S_n^{-1}$ \;
    $\textbf{m}_n = \textbf{m}_{n-1} + \textbf{K}_n [y_n - \textbf{X}_n \textbf{m}_{n-1}]$ \;
    $\textbf{P}_n = \textbf{P}_{n}^{-} - \textbf{K}_n \, S_n \, \textbf{K}_n^T $ \;
  }

}
Reverse whitening transformation for $\mathbf{m}_N$ \;
\KwRet{$\mathbf{m}_N$}
\caption{Recursive Least Squares with Drift Model}
\label{RLS:basic}
\end{algorithm}

During our research we did not find a good state transition model, therefore
we only use the update step of the Kalman filter \cref{KF_update}.
We tried some momentum based approaches but
did not receive any promising results. 
Therefore our approach is a special case of the Kalman filter
equations with
an Identity matrix as the state transition matrix $\mathbf{A}$.
Exploring different state transition models and thereby incorporating
the prediction step of the Kalman filter is part of future work.

Our final approach for recursive surrogate-modeling is summarized
in \Cref{RLS:basic}.
The RLS with drift model algorithm and the ridge regression
from \cref{sec:ridge} can easily be combined with the MORE algorithm,
for this only the part for learning the quadratic 
surrogate model $\hat{f}$ has to be changed in the original MORE
(\Cref{alg:more_appendix}).


%\begin{algorithm}[H]
%\renewcommand{\algorithmcfname}{Algorithm}
%\DontPrintSemicolon
%\SetAlgoLined
%\KwIn{Parameters $\epsilon$ and $\beta$, initial search distribution $\pi$ \;
%  \\ $K$ number of iterations,  $N$ samples per iteration}
%\For{$k = 1,...,K$}
%{
%  \For{$n = 1,...,N$}
%  {
%    Draw sample $\mathbf{x}_n \sim \pi$\;
%    Evaluate $\mathbf{x}_n$ on objective function $f(\mathbf{x}_n) = y_n$\;
%  }
%  Learn the quadratic model $\hat{f}$ \;
%  Solve  $\text{argmin}_{\eta >0, \omega > 0} \, g(\eta, \omega)$
%  using \Cref{eq:dual} \;
%  Update the search distribution $\pi$ using \Cref{policy_update}\;
%}
%\caption{MORE}
%\label{alg:more}
%\end{algorithm}