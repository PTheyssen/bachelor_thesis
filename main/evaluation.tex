% !TeX spellcheck = en_US
% !TeX encoding = UTF-8
% !TeX root = ../thesis.tex

\chapter{Evaluation}
In this chapter we first describe the setup for our experiments.
%and list all hyperparameters for our algorithm.
Then we discuss the Rosenbrock function and the planar
reaching task and present the best performing
configuration of our algorithm for each task.
% Afterwards we review various results from our
% hyperparameter search.

% things to benchmark
% - loss
% - sample efficiency
% - run time

\section{Setup}
All algorithms were implemented in python using
NumPy\footnote{\href{https://numpy.org/}{https://numpy.org/}}.
The optimization algorithm for MORE is Low-storage BFGS
from NLopt\footnote{\href{https://nlopt.readthedocs.io/en/latest/}
{https://nlopt.readthedocs.io/en/latest/}}.
The clusterwork2 (cw2) framework, developed at the
Autonomous Learning Robots (ALR) Lab, was used for
running experiments in a convenient way.
The experiments were conducted on a machine with two 8-core
AMD Ryzen 2700X processors clocked at 2.6 GHz and 31GB of RAM.


\section{Experiments}
The focus in our experiments is on the sample efficiency of the
MORE algorithm. While the original MORE algorithm achieves
state-of-the-art results in terms of optimization accuracy
the surrogate model estimation is data-intensive.
The metric we focus on is sample efficiency for which we use the heuristic
$s = 4 + \lfloor 3 \log(n) \rfloor$ of the CMA-ES
algorithm \citep{hansen2016cma}, with $n$ being the dimension
of the problem and $s$ the number of samples drawn from the
search distribution in each iteration.

We compare our recursive least squares (RLS) appraoch
to the ridge regression (RR) and
the original MORE algorihtm, which uses Bayesian dimensionality
reduction approach combined with linear regression (BLR).
Generally, we use the
CMA-ES heuristic to determine the number of samples.
As we are working with a stochastic search algorithm we do
10 runs with each approach and calculate the \textit{arithmetic} mean.

% TODO: skip or really use systematic data in appendix
% For an exhaustive hyperparameter search and a listing of the obtained results
% see \Cref{appendix:par_search}.

\subsection{Test Functions for Optimization}
\label{sec:test_func}
Test functions provide artificial landscapes to evaluate the
performance of optimization algorithms. They can be used to
assess robustness, convergence speed and general performance of the
algorithms \citep{molga2005test}.

The Rosenbrock function (\Cref{fig:rosenbrock}) is uni-modal and convex.
It has one global optimum which is inside a parabolic shaped flat valley.
It is usually easy to find the valley,
however finding the global optimum is particularly difficult,
which makes this function popular for testing the performance
of algorithms.
It is defined as 
\begin{equation*}
 f(x) = \sum^{n-1}_{i=1} [100(x_{i+1} - x_i^2)^2 + (1-x_i)^2].
\end{equation*}

\begin{figure}[ht!]
    \centering
    % \includesvg[width=0.4\textwidth]{figures/rosenbrock_function}
    \caption{2D Rosenbrock function}
    \label{fig:rosenbrock}
\end{figure}

The function has a global minimum at zero $(f(\mathbf{x}) = 0)$.
In our experiments we initialize the mean of the search distributions
randomly. We test our algorithm on the 15 dimensional
Rosenbrock function, the results are shown in \Cref{fig:rosen_result}.

% TODO add result on rosenbrock 8 dim on the side
% TODO look for styling plots --> make them look better
\begin{figure}[ht!]
     \centering
     % \input{figures/rosenbrock15dim}
     \hspace{1cm}                       
     \caption{
       This figure shows the mean of 10 runs for RLS, LS and
       the original MORE algorithm, which uses
       Bayesian Linear Regression (BLR),
       on 15 dimensional Rosenbrock.
     }
     \label{fig:rosen_result}  
\end{figure}

% minipage example
%\begin{figure}
%    \centering
%    \begin{minipage}{0.45\textwidth}
%      \centering
%      \includegraphics[width=1\textwidth]{figures/5dim}
%      \hspace{1cm}                       
%      \caption{5 dimensional Rosenbrock}
%      \label{fig:5dim}
%    \end{minipage}\hfill
%    \begin{minipage}{0.45\textwidth}
%      \centering
%      \includegraphics[width=1\textwidth]{figures/15dim}
%      \hspace{1cm}                       
%      \caption{15 dimensional Rosenbrock}
%      \label{fig:15dim}
%    \end{minipage}
%  \end{figure}

\subsection{Planar Reaching Task}
Policy search methods enable us to work with parameterized policies,
therefore we can use a policy representation that is relevant
to a given task.
In robotics Dynamic Movement Primitives 
(DMPs) first introduced in \citet{ijspeert2002learning} are commonly used for movement tasks.
Formalized as second-order dynamic systems, DMPs offer a compact
representation for movements, allowing to choose between a rhythmic
and a discrete movement. 

For our task we use a 5 link robot with DMPs as the underlying
control policy. The end-effector of the robot has
to reach a via-point $v_{100} = [1,1]$ at time step 100
and at the final time step $T = 200$ the via-point $v_{\, 200} = [5,0]$.
The reward is given by a quadratic cost term for
the two via-points, for self-collision a penalty of 100 is given.
As the the via-points are defined in end effector space the objective
function is highly non-quadratic in the parameters. Using
5 basis functions per degree of freedom for the DMPs results
in a 25 dimensional parameter vector.
In \Cref{fig:reaching_result} we see the average reward and the resulting
movement.

\begin{figure}
    \centering
    \begin{minipage}{0.45\textwidth}
      \centering
      % \input{figures/reaching_results}
      \hspace{1cm}                       
    \end{minipage}\hfill
    \begin{minipage}{0.45\textwidth}
      \centering
      \input{figures/via_movement}
      \hspace{1cm}                       
    \end{minipage}
     \caption{\small
       {\color{gray!25!black!60} Left:}
       Results on via-point reaching task
       LS approach, RLS approach with and without a pool.
       The LS approach performs slightly better than RLS
       with pool. The RLS without a pool finds
       acceptable solutions to the tasks, but performs worse
       that the other approaches.
       {\color{gray!25!black!60} Right:} Resulting movement for
       via-point reaching task of the RLS with pool.
       The via-points are indicated by the red crosses. The
       postures of the resulting motion are shown as overlay.
       Darker postures are later in time.
     }
     \label{fig:reaching_result}      
\end{figure}

% \subsection{Mujoco Reaching Task}
% Using mujoco we use a simulated reaching task.

% \subsection{Hyper parameter Search}
% - do this better --> compare ALGO II theses
% - do great graphics, tables, figures for this
% - try doing a sweep with wandb (or manually)
% - research how to do good algorithm engineering
% make table of parameters to tune
% look for best way to show results (table with highlighted numbers)

\subsection{Ball in a Cup}
The motor skill game Ball-in-a-Cup consists
of small cup that has a string with a ball attached to it.
Initially the ball hangs down vertically in a resting position.
The goal is to toss the ball up and catch it inside the cup.
The reward is calculated as the distance from the center of the cup
to the ball. 
We ran a simulation with the Barret WAM robot arm with three
degree of freedom (DoF) using the
MuJoCo\footnote{\href{http://www.mujoco.org/}{http://www.mujoco.org/}}
physics engine.
\begin{figure}
    \centering
    \begin{minipage}{0.5\textwidth}
      \centering
      \includegraphics[scale=0.7]{figures/ball_in_a_cup.png}
      \hspace{1cm}                       
    \end{minipage}\hfill
    \begin{minipage}{0.45\textwidth}
      \centering
      \hspace{1cm}                       
    \end{minipage}
     \caption{\small
       {\color{gray!25!black!60} Left:}
       Picture of the task in simulation framework.       
       {\color{gray!25!black!60} Right:}
       Results from running RLS on Ball-in-a-Cup.       
     }
     \label{fig:reaching_result}      
\end{figure}

%\subsection{Further Investigations}
%In our experiments we have to deal with unsmooth reward functions
%due to penalties. For our batch solution we normalize our data
%using the standard score, for this we use the we calculate
%the mean and variance of the current batch of rewards.
%
%
%- tried exponential moving average, the goal was
%to have the same mean as using a pool for normalization
%
%- show plots from notion