% !TeX spellcheck = en_US
% !TeX encoding = UTF-8
% !TeX root = ../thesis.tex

\chapter{Recursive Surrogate-Modeling for MORE}
In this chapter we will first motivate the idea of recursively
estimating the surrogate model for the MORE algorithm.
We formulate the surrogate model estimation task
as a regression problem and present our solution to it.
Finally we connect our approach with the MORE algorithm.

\section{Surrogate Model Estimation}
\label{sec:surrogate}
\subsection{Motivation}
Previously the surrogate model has been estimated from scratch in each
iteration using samples and corresponding
values of the objective function. 
However, subsequent models are correlated
due to the locality of the data, see \cref{fig:sur_model} for an example.
\begin{figure}[t]
  \centering
  \subfigure[][objective function]{\input{figures/rosenbrock_mean}}
  \subfigure[][surrogate model (iteration $i$)]{\input{figures/model_1}}
  \subfigure[][surrogate model (iteration $i+1$)]{\input{figures/model_2}}
  \caption{\small
    In (a) the original objective function,
    a 2 dimensional Rosenbrock function (\cref{sec:test_func}) is shown.
    In (b) and (c) we see the search distribution mean,
    the surrogate model and the samples used to estimate
    the model from two subsequent MORE iterations.}
 \label{fig:sur_model}
\end{figure}
This arises from the fact that the KL-divergence is
used to bound the distance between
subsequent search distributions. Therefore recursive estimation
techniques may be able to utilize the information contained in the previous
surrogate model and thus reduce the total amount of samples needed.
Additionally this may also reduce the runtime of the algorithm.

The surrogate model has the following form
\begin{align*}
  % TODO: explain why using 1/2 or not?
  f(\mathbf{x}) \approx \hat{f}(\mathbf{x}) =
  %-\frac{1}{2}
  \mathbf{x}^T \mathbf{R} \mathbf{x}
  + \mathbf{x}^T \mathbf{r} + r.
\end{align*}
where $f(\mathbf{x}) : \mathbb{R}^n \rightarrow \mathbb{R}$
is the original objective function.
The surrogate model $\hat{f}(\mathbf{x})$
has a quadratic term $\mathbf{R} \in \mathbb{R}^{n \times n}$,
a linear term $\mathbf{r} \in \mathbb{R}^{n}$ and a scalar $r$.
Using a quadratic model is sufficient as the exponential of the
Gaussian is also quadratic in the parameters.
A more complex model could not be exploited by
a Gaussian distribution \citep{abdolmaleki2015model}.
Nevertheless estimating the surrogate model is a challenging task as we
have to estimate $\mathcal{O}(n^2)$ many parameters while using a minimal
amount of samples in each MORE Iteration.


% TODO: include 2D contour line or 3d version of subsequent models

\subsection{Regression Problem}
We can formulate the task of estimating the surrogate model as
a regression problem (\ref{eq:regression}). For this we use
a feature function $\phi(\mathbf{x})$ which returns
a bias term, all linear and all quadratic terms. The dimensionality of
$\phi(\mathbf{x})$ is $D = 1 + d + d(d + 1) / 2$, where $d$
is the dimensionality of the parameter space.
\begin{align}
  \label{eq:regression}
  y = \phi(\mathbf{x}) \beta + \epsilon
\end{align}
% The surrogate model parameters are in $\beta$
% a vector of length $D$ containing first the
% scaler than $d$ linear terms and then $d(d+1) /2$ quadratic terms
% (the lower triangular matrix).
Our data consists of the samples and corresponding objective
values $\mathcal{D} = \{(\mathbf{x}_1, y_1),\dots,(\mathbf{x}_n, y_n)\}$.
To solve the regression problem we set up
the design matrix $\mathbf{X}$ as depicted in \cref{eq:lgs}.
One row of $\mathbf{X}$ contains a one as the first entry for the bias term.
Then the next $d$ entries are made up of the corresponding
sample $\mathbf{x}_k$ for the linear term.
The final $d(d + 1) / 2$  entries are
the lower triangular matrix of the product $\mathbf{x}_k \mathbf{x}_k^T$,
which we denote by tril$(\mathbf{x}_k \mathbf{x}_k^T)$.
% Corresponding to the set
% $\text{tri}(\mathbf{x}) = \{x_1^2 , x_1 x_2 , x_1 x_3 ,
% \cdots , x_2^2 , x_2 x_3 \cdots\}$,
% where we multiply each sample entry with itself and all entries with
% higher index. 

For the least squares (LS) approach with $n \geq D$ samples
we get the following system of linear equations

\begin{equation}
  \label{eq:lgs}
\begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{bmatrix}
=
\underbrace{
\begin{bmatrix}
  1 & \mathbf{x}_1 & \text{tril}(\mathbf{x}_1 \mathbf{x}_1^T) \\
  1 & \mathbf{x}_2 & \text{tril}(\mathbf{x}_2 \mathbf{x}_2^T) \\
  \vdots & \vdots & \vdots \\
  1 & \mathbf{x}_n & \text{tril}(\mathbf{x}_n \mathbf{x}_n^T) \\
\end{bmatrix}}_{\mathbf{X}}
\begin{bmatrix}
  \beta_1 \\ \beta_2 \\ \vdots \\ \beta_D
\end{bmatrix}
+
\begin{bmatrix}
  \epsilon_1 \\
  \epsilon_2 \\
  \vdots \\
  \epsilon_n
\end{bmatrix}.
\end{equation}

We assume that the measurement noise $\epsilon_k$ is zero mean Gaussian
$\epsilon_k \sim N(0, \sigma^2)$ distributed.
\Cref{eq:lgs} can be solved with ordinary least squares
$\beta = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X} \mathbf{y}$.
For our experiments we use a form of ridge regression \citep{hoerl1975ridge}
for comparison to our recursive estimation approach.


The solution vector $\beta$ contains the surrogate model parameters
$(r, \mathbf{r}, \mathbf{R})$.


Using recursive estimation we
can process each pair of samples and rewards $(\mathbf{x}_k, y_k)$ one
at a time.
$$
 y_k =
 \big(1 \;  \mathbf{x}_k \; \text{tril}(\mathbf{x}_k \mathbf{x}_k^T) \big)
\begin{pmatrix}
  \beta_1 \\ \beta_2 \\ \vdots \\ \beta_D
\end{pmatrix} 
+ \epsilon_k
$$
\section{Recursive Least Squares}
As we did not find a good state transition model
we only use the update step of the Kalman filter \cref{KF_update}.
We tried some momentum based approaches during our evaluation but
did not receive any promising results. 
Therefore our approach is a special case of the Kalman filter
equations with
an Identity matrix as the state transition matrix $\mathbf{A}$.
We call our approach recursive least squares with drift model
for the parameters (\Cref{RLS:basic}).

Let us now explain how we use the recursive least squares (RLS) algorithm
to compute the solution vector $\beta$ to the regression problem.
As the model parameters do not stay constant we assume they perform
a Gaussian random walk between measurements and for an arbitrary
step $k$ we get
\begin{align*}
  p(y_k | \, \mathbf{\beta}_k) &= \mathcal{N}(y_k | \, \mathbf{H}_k \mathbf{\beta}_k, \sigma^2) \\
  p(\mathbf{\beta}_k | \, \mathbf{\beta}_{k-1}) &= \mathcal{N}(\mathbf{\beta}_k | \,
                               \mathbf{\beta}_{k-1}, \mathbf{Q}) \\
  p(\mathbf{\beta}_0) &= \mathcal{N}(\mathbf{\beta}_0 | \, \mathbf{m}_0, \mathbf{P}_0)
\end{align*}
where $\mathbf{Q}$ is the covariance of the random walk.
The parameter noise is added to the covariance matrix of the parameters
before each update step.
Our prior $\mathbf{m}_0$ is chosen such that for the
first surrogate model parameters
we set the quadratic term to an identity matrix $\mathbf{R} = \mathbf{I}$
and the other terms to zero $ \mathbf{r} = \mathbf{0}$ and $ r = 0$.
The covariance matrix is initialized as
$\mathbf{P}_0 = \delta \mathbf{I}$ with
the parameter $\delta$ controlling how confident
we are in our prior.
We want to compute the filtering distribution
\begin{align*}
  p(\beta_k | \, y_{1:k}, \mathbf{x}_{1:k})
  &\propto p(y_k, \mathbf{x}_k | \, \beta_k) \,
    p(\beta_k |\, y_{1:k-1}, \mathbf{x}_{1:k-1}) \\
  &\propto \mathcal{N}(\beta_k | \, \mathbf{m}_k, \mathbf{P}_k), 
\end{align*}
for this we can use the update equation
of the Kalman filter (\ref{KF_update}).
Increasing the covariance matrix of the parameters
by the model noise combined with
the Kalman filter update step
gives us \cref{RLS:basic},
which computes the model parameters in a recursive fashion.


\begin{algorithm}[H]
\renewcommand{\algorithmcfname}{Algorithm}
\SetKw{KwInit}{Initialization:}
\DontPrintSemicolon
\SetAlgoLined
\KwIn{% $\{(\mathbf{x}_1,y_1),\cdots,(\mathbf{x}_N,y_N)\}$
  stream of samples as rows of design matrix $\mathbf{X}_n$
  and rewards $y_n$ with $n = 1, \dots, N$, \\
  $\mathbf{Q}$ model noise, $\sigma^2$ measurement noise}
\KwInit{$\mathbf{m}_0 = (0, \mathbf{0}, \mathbf{I})$,  $\mathbf{P}_0 = \delta \mathbf{I}$}

\For{$n = 1,...,N$}
{
  \Begin(Adding the model noise)
  {
    $\mathbf{P}_n^- = \mathbf{P}_{n-1} + \mathbf{Q}$ \;
  }
  $~$ \;
  \Begin(Update step)
  {
    $S_n = \mathbf{X}_n \; \mathbf{P}_n^- \, \mathbf{X}_n^T + \sigma^2$ \;
    $\textbf{K}_n = \textbf{P}_{n}^{-} \, \textbf{X}^T_n \, S_n^{-1}$ \;
    $\textbf{m}_n = \textbf{m}_{n-1} + \textbf{K}_n [y_n - \textbf{X}_n \textbf{m}_{n-1}]$ \;
    $\textbf{P}_n = \textbf{P}_{n}^{-} - \textbf{K}_n \, S_n \, \textbf{K}_n^T $ \;
  }
}
\caption{Recursive Least Squares with Drift Model}
\label{RLS:basic}
\end{algorithm}
  
\subsection{Sample pool}
The original MORE algorithm uses a sample pool, which contains samples
from previous iterations, for estimating the surrogate model.
In theory when using a recursive estimation approach
the information in a sample pool is redundant.

In our experiments we did not receive good results when
using only new samples without a sample pool, since
the predictions for the surrogate models are to inaccurate.
The first adjustment is to use a warm start, meaning we get
a accurate prediction in the first iteration by processing
a large batch of samples. This prediction can then be subsequently
updated.

On optimization test functions like the Rosenbrock
function \cref{eq:rosenbrock} we also did not receive
promising results using a warm start.
Thus we also tried using a sample pool for our recursive estimation approach.
From a theoretical standpoint this is rather imprecise,
this improved our results regardless.
When using a sample pool we generally process the newest samples
last.
Additionally we explored giving older samples 
a greater model noise and by this encoding our uncertainty about them.
For this we simply introduce a counter for each sample,
indicating how many times the sample has been used.
For older samples (higher counter) we increased the model noise controlled
by a weight factor $\gamma$.
When using this sample weighting we increase the model noise in the
RLS algorithm by computing
$$ \mathbf{Q} + \gamma \,\cdot \text{counter} \, \cdot \mathbf{I} $$
before adding the model noise to the covariance
matrix of the parameters.
This improved our results on the Rosenbrock test function compared
to the RLS version with constant model noise for all samples.
Still the use of a sample pool is theoretically unsound and part
of future work is to explore ways to avoid it.

%\subsection{Normalization}
%We explored different ways of normalization. First a simple
%standard score.
%
%We also tested exponential weighting normalization.
%
%- exponential weighting (search for citation)
%
%- standard score
%
\subsection{Whitening}
We used \textit{Cholesky whitening} which is based on
Cholesky factorization of the precision matrix
$\mathbf{L}\mathbf{L}^T = \Sigma^{-1}$, which leads to the whitening
matrix $\mathbf{W}^{\text{Chol}} = \mathbf{L}^T$.
If the Cholesky whitening is unsuccessful due to numerical problems,
we instead standardize
the random vector meaning $\text{var}(\mathbf{z}) = 1$ but the correlations
are not removed.
In \cref{fig:whitening} we provide an example illustrating the effect of 
whitening for our parameter estimation task.

\begin{figure}[t]
  \centering
  \subfigure[][without whitening]{\input{figures/LS_no_whitening}}
  \subfigure[][in whitened space]{\input{figures/LS_whitening}}
  \subfigure[][unwhitened parameters]{\input{figures/LS_unwhitened}}
  \caption{\small
    Example of using MORE with a least squares approach for
    surrogate-modeling with and without whitening on the 2-dimensional
    Rosenbrock function (\cref{sec:test_func}).
    The plots show the predicted surrogate model
    parameters $\mathbf{\beta}$. In (a) the parameters are estimated
    without whitening the data. In (b) a whitening transformation is
    applied to the data before parameter estimation
    and (c) shows the parameters from (b) after reversing
    the whitening transformation.
    We can see that the parameters in whitened space
    stay in a smaller range 
    making the estimation task easier.}
 \label{fig:whitening}
\end{figure}


\section{MORE with Recursive Surrogate-Modeling}
We now present the MORE algorithm with recursive surrogate-modeling.
First we initialize the search distribution 
$\pi(\mathbf{x}) = \mathcal{N}(\mathbf{x} |\, \mu, \Sigma)$ and
the RLS algorithm. In the robot
learning setting the search distribution may be intialized
through imitation learning.
In each iteration the search distribution is then used to create
samples which are evaluated on the objective function.
The samples are used to create the design matrix $\mathbf{X}$
(as in \ref{eq:lgs}).
Data preprocessing techniques like whitening (\cref{sec:whitening})
and normalization
may be apllied to the design matrix and rewards before they
are given to the RLS algorithm to learn the surrogate model, which in turn
is used to compute the update to the search distribution in closed form.
MORE with Recursive Surrogate-Modeling is summarized in \Cref{alg:more}.

\begin{algorithm}[H]
\renewcommand{\algorithmcfname}{Algorithm}
\DontPrintSemicolon
\SetAlgoLined
\KwIn{Parameters $\epsilon$ and $\beta$, \\ $K$ number of iterations,
  $N$ samples per iteration}
\Begin(Initialization:)
{
  Initialize search distribution $\pi$ \;
  Initialize RLS with $\mathbf{m}_0, \mathbf{P}_0$ \;
}
\For{$k = 1,...,K$}
{
  \For{$n = 1,...,N$}
  {
    Draw sample $\mathbf{x}_n \sim \pi$\;
    Evaluate $\mathbf{x}_n$ on objective function $f(\mathbf{x}_n) = y_n$\;
  }
  \Begin(Estimate quadratic model $\hat{f}$)
  {
    \For{$n = 1,...,N$}
    {
      Whitening: $\mathbf{X}_n = \mathbf{W} \, \mathbf{X}_n$\;
      % TODO: normalization maybe in fundamentals
      % Optionally: Use normalization\;
      Optionally: Increase model noise for older samples\;      
      Compute surrogate model parameters using \Cref{RLS:basic} \;
      % RLS$(\mathbf{X}_n, y_n, \mathbf{Q}, \sigma^2)$ \;
    }
  }
  Solve  $\text{argmin}_{\eta >0, \omega > 0} \, g(\eta, \omega)$
  using \Cref{eq:dual} \;
  Update search distribution $\pi$ using \Cref{policy_update}\;
}
\caption{MORE with Recursive Surrogate-Modeling}
\label{alg:more}
\end{algorithm}

%\subsection{Moving back to prior}
%One technique we investigated is 
%- to prior calculation (optimal for whitened space)
%
%- set up equation
%
%\subsection{State Transition Model}
%Let us quickly state the full Kalman filter equations, the
%derivation can be found in the appendix (TODO ref)
%
%Prediction step:
%
%Update step:
%
%Introducing an accurate state transition model to incorporate the
%prediction step of the kalman filter would be ideal.
%We tried some simple momentum approaches based on the differences of
%subsequent surrogate model parameters, but those did not yield
%good results.
%Our Recursive least squares algorithm is a special case of these
%equations using an state transition model matrix $\mathbf{A} = \mathbf{I}$.
%