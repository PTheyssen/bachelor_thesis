% !TeX spellcheck = en_US
% !TeX encoding = UTF-8
% !TeX root = ../thesis.tex

\chapter{Recursive Surrogate-Modeling for MORE}
In this chapter we will first motivate the idea of using recursive filtering
for estimating the surrogate model. Then we formulate the surrogate estimation
as a regression problem and derive the recursive
least squares approach for the drift model.
Then we formulate our version of the MORE algorithm with
recursive estimation of the surrogate model.
Next we discuss various data preprocessing techniques we use or explored.

\section{Motivation}
So far, the surrogate model has been estimated from scratch in each
iteration using the samples and objective values. 
Real world samples are very costly we want to be sample efficient.
Further subsequent models are correlated
(TODO: add Figure of surrogate model) due to the locality of the
data, because the KL-divergence is used to bound the distance between
subsequent search distributions. Therefore recursive estimation
techniques may be used to exploit the information of previous
surrogate models and thus reduce the total amount of samples needed.

Previously estimating the surrogate model for the MORE algorithm has been solely based
on samples and corresponding rewards. There was a sample pool used to be more
sample efficient. By using an recursive filtering approach
we want to utilize the information of previous surrogate models instead
of using only samples and rewards for estimation.

Since the MORE algorithm uses the KL-bound to limit the distance from the previous search
distribution the subsequent surrogate models are also locally correlated.

The surrogate model has the form
\begin{align}
  \label{surrogate}
  \mathbf{M}(\mathbf{x}) = -\frac{1}{2} \mathbf{x}^T A \mathbf{x}
  + \mathbf{x}^T b + c 
\end{align}
With a quadratic term $A$, a spd(semi positive definite) matrix, a
linear term $b$ and a scalar $c$.

% TODO: include 2D contour line or 3d version of subsequent models


The learned quadratic models are locally correlated and thus we can use recursive
filtering algorithms to exploit the information of previous models at
the current time step, this has the potential to result in using less samples
and also reducing the overall runtime of the algorithm.

\section{Surrogate Model Estimation}

\subsection{Regression problem}

If we formulate this as a regression problem we get:
\begin{align*}
  y_k &=  \mathbf{x}^T \mathbf{A} \mathbf{x} + \mathbf{x}^T \mathbf{b} + c + \epsilon_k
\end{align*}

So we want to estimate the surrogate model parameters $\theta = (\mathbf{A}, \mathbf{b}, c)$ from the samples $\mathbf{x}$
and the rewards $y$. We also assume that
the measurement noise $\epsilon_k$ is zero mean Gaussian $\epsilon_k \sim N(0, \sigma^2)$ distributed.

In our algorithm we solve this problem by using the Recursive Least squares equations \Cref{RLS}.


\section{Recursive Least Squares}
Recursive least squares with drift model for estimated parameters
as special case of Kalman Filter
$$ p(\theta_k | y_{1:k-1}) = \text{N}(\theta_k | m_k^{-}, P_k^{-}) $$

with
\begin{align}
  m_k^{-} &= m_{k-1} \\
  P_k^{-} &= P_{k-1} + Q
\end{align}

Now we simply replace $P_{k-1}$ with $P_k^{-}$ in our update equations:
\begin{align*}
     S_k &= \textbf{H}_k \textbf{P}_{k}^{-} \textbf{H}^T_k + \sigma^2 \\
     \textbf{K}_k &= \textbf{P}_{k}^{-} \textbf{H}^T_k S_k^{-1} \\
     \textbf{m}_k &= \textbf{m}_{k}^{-} + \textbf{K}_k [y_k - \textbf{H}_k \textbf{m}_{k}^{-}] \\
     \textbf{P}_k &= \textbf{P}_{k}^{-} - \textbf{K}_k S_k \textbf{K}_k^T
\end{align*}

\section{Data Preprocessing Techniques}
TODO: move this whole section into evaluation/experiments?

To improve the performance of the algorithm we examined several data processing techniques.
Here talk theoretically about the methods and in evaluation part
show experiments about different techniques (for example show whitened model
parameters)

\subsection{Whitening}
Whitening is common data preprocessing method in statistical analysis
to transform a correlated random vector into an uncorrelated one
\citet{kessy2018optimal}.

We employ whitening for our algorithm, because uncorrelated
random variables often greatly simplify  multivariate data analysis (cite).

\textit{Whitening} is a linear transformation that converts a $d$-dimensional
random vector $\mathbf{x} = (x_1,...,x_d)^T$ with mean
$\text{E}(\mathbf{x}) = \mathbf{\mu} = (\mu_1,...,\mu_d)^T$ and
positive definite $d \times d$ covariance matrix
var$(\mathbf{x}) = \Sigma$ into a new random vector
\begin{align}
  \label{whitening}
 \mathbf{z} = (z_1,...,z_d)^T = \mathbf{W}\mathbf{x}
\end{align}

of the same dimension $d$ and with unit diagonal ``white'' covariance
var$(\mathbf{z}) = \mathbf{I}$. The square $d \times d$
matrix $\mathbf{W}$ is called the whitening matrix.

The whitening transformation defined in Equation \Cref{whitening} requires
the choice of a suitable whitening matrix $W$.
Since $\text{var}(z) = \mathbf{I}$ it follows that
$\mathbf{W}\Sigma \mathbf{W}^T = \mathbf{I}$ and thus
$\\mathbf{W}(\Sigma \mathbf{W}^T\\mathbf{W}) = \mathbf{W}$, which
is fulfilled if $\mathbf{W}$ satisfies the condition
$$ \mathbf{W}^T \mathbf{W} = \Sigma^{-1} $$

This constrain does not uniquely determine the whitening
matrix $\mathbf{W}$, instead given $\Sigma$ there are infinitely many
possible matrices $\mathbf{W}$, because it allows for rotational freedom.

We used \textit{Cholesky whitening} which is based on
Cholesky factorization of the precision matrix
$\mathbf{L}\mathbf{L}^T = \Sigma^{-1}$, which leads to the whitening
matrix:
$$ \mathbf{W}^{\text{Chol}} = \mathbf{L}^T $$.

If the cholesky whitening is unsuccessful we fall back to standardizing
the random vector meaning $\text{var}(\mathbf{z}) = 1$ but the correlations
are not removed.

- TODO: include example samples before and after whitening

\subsection{Sample pool}
The original MORE algorithm and other policy search algorithms (REPS) we
use a sample pool.

Theoretically  we would want to avoid using a sample pool and instead use
the information form past samples in the form of the previously predicted
model parameters. But with our algorithm using no sample pool led
to unsatisfactory estimation results.

We increased the model noise for older samples, encoding the fact that
older samples should have a higher covariance, meaning we are less
certain about them. We implemented this by simple adding a constant
amount to the covariance of the RLS equations for each time a sample is
used. For higher dimensional task this is around 5-10 times.
Still the use of a sample pool is theoretically unsound and problematic,
instead a accurate state transition model could be introduced.

\subsection{Higher model noise for older samples}
Using the recursive least squares algorithm we had to maintain a sample
pool, because otherwise the resulting estimation was not
satisfactory. From a theoretic perspective this sloppy and ideally we
would refrain from using the sample pool.

We introduced a simple counter for each sample, indicating how many times
the sample has been used.
Then for older samples (higher counter) we increased the model
noise proportionally.
We tried a linear relationship between the counter and
the increased covariance for the model parameters.

$$ new\_cov = constant\_cov\_matrix + w * \text{counter} * \mathbf{I} $$

- TODO: try different weightings: exponential

This improved our results on the rosenbrock test function compared
to the RLS version with constant model noise for all samples.
In each MORE iteration first the oldest samples from the sample pool
are used, meaning the recent samples with lowest model noise are
processed last by RLS.
\subsection{Normalization}
Original MORE approach uses standard score normalization, doing it
in a batch way at each iteration for all samples in the sample pool.

This is not fit for recursive estimation.
- different types of reward functions difficulties:
- rosenbrock (high range of values)
- sharp spikes, in rewards (punishing term for reaching task)

- simply online calculation of mean and var for normalization is not
a good fit (include plots and results of investigation)

- instead use moving average to calculate mean and var only of window (discard old
  data) 

- describe moving average computation of normalization

\section{MORE with Recursive Surrogate-Modeling}
\begin{algorithm}[H]
\DontPrintSemicolon
\SetAlgoLined
\KwIn{Parameters $\epsilon$ and $\beta$, initial distribution, \\ $K$ number of iterations, $N$ samples per iteration}

\For{$k = 1,...,K$}
{
  \For{$n = 1,...,N$}
  {
    Draw parameters $\theta_n \sim \pi$\;
    Execute task with $\theta_n$ and receive $R(\theta_n)$\;
  }
  \Begin(Learn quadratic model with Recursive Least Squares)
  {
    \For{$n = 1,...,N$}
    {
      Whitening: $\mathbf{W}\theta_n$\;
      Increase model noise for older samples\;
      Compute surrogate model parameters with RLS\;
    }
  }
  Minimize dual function $g(\eta, \omega)$ using Eq.\;
  Update search distribution $\pi$ using \Cref{policy_update} \;
}
\caption{MORE Algorithm with Recursive Surrogate-Modeling}
\end{algorithm}

%\subsection{Moving back to prior}
%One technique we investigated is 
%- to prior calculation (optimal for whitened space)
%
%- set up equation
%
%\subsection{State Transition Model}
%Let us quickly state the full Kalman filter equations, the
%derivation can be found in the appendix (TODO ref)
%
%Prediction step:
%
%Update step:
%
%Introducing an accurate state transition model to incorporate the
%prediction step of the kalman filter would be ideal.
%We tried some simple momentum approaches based on the differences of
%subsequent surrogate model parameters, but those did not yield
%good results.
%Our Recursive least squares algorithm is a special case of these
%equations using an state transition model matrix $\mathbf{A} = \mathbf{I}$.
%