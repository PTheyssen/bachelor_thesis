% !TeX spellcheck = en_US
% !TeX encoding = UTF-8
% !TeX root = ../thesis.tex

\chapter{Recursive Surrogate-Modeling for MORE}
In this chapter we will first motivate the idea of recursively
estimating the surrogate model for the MORE algorihtm.
We formulate the surrogate model estimation
as a regression problem and present our solution.
Then we combine our appoach with the MORE algorithm
Finally we discuss various data preprocessing techniques.

\section{Surrogate Model Estimation}

\subsection{Motivation}
So far, the surrogate model has been estimated from scratch in each
iteration using samples and corresponding
values of the objective function. 
However, subsequent models are correlated
(TODO: add Figure of surrogate model) due to the locality of the
data. This arises from the fact that the KL-divergence is
used to bound the distance between
subsequent search distributions. Therefore recursive estimation
techniques may be able to utilize the information of previous
surrogate models and thus reduce the total amount of samples needed.
Additionaly this may also reduce the runtime of the algorithm.

The surrogate model is a quadratic model of the form
\begin{align}
  \label{surrogate}
  \mathcal{R}(\mathbf{x}) = -\frac{1}{2} \mathbf{x}^T \mathbf{R} \mathbf{x}
  + \mathbf{x}^T \mathbf{r} + r 
\end{align}
With a quadratic term $\mathbf{R} \in \mathbb{R}^{n \times n}$,
a linear term $\mathbf{r} \in \mathbb{R}^{n}$ and a scalar $r$,
here $n$ denotes the dimension of the objective function.
Using a quadratic model is sufficient as the exponential of the
Gaussian is also quadratic and hence could not utilize information
from more complex models \citep{abdolmaleki2015model}.


% TODO: include 2D contour line or 3d version of subsequent models

\subsection{Regression problem}
If we formulate this as a regression problem we get, by forming
a feature function $\phi(\mathbf{x})$ which returns all
a bias term, all linear and all quadratic terms. The dimensionalty of
$\phi(\mathbf{x})$ is $D = 1 + d + d(d + 1) / 2$, where $d$
is the dimensionality of the parameter space.
\begin{align*}
  y = \phi(\mathbf{x}) \beta + \epsilon
\end{align*}
The surrogate model parameters are in $\beta$
a vector of length $D$ containing first the
scaler than $d$ linear terms and then $d(d+1) /2$ quadratic terms
(the lower triangular matrix).

Our data are the samples and correspondings objective function
values $\mathcal{D} = \{(\mathbf{x}_1, y_1),...,(\mathbf{x}_n, y_n)\}$.
To solve we regression problem we set up
the design matrix $\mathbf{X}$ which contains 
ones for the bias term, than the sample for linear term and then
$d(d+1) /2$ terms for quadratic model in each row.

For the Least squares approach with $n \geq D$ samples
we get the following system of linear equations. 
$$
\begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{bmatrix}
=
\underbrace{
\begin{bmatrix}
  1 & x_1^1 & \cdots & x_d^1  & x_{d+1}^1 & \cdots &  x_{D}^1 \\
  1 & x_1^2 & \cdots &  x_d^2 & x_{d+1}^2 & \cdots &  x_{D}^2 \\
  \vdots  & \vdots & \ddots & \vdots &  \vdots & \ddots & \vdots \\
  1 & x_1^n & \cdots &  x_d^n & x_{d+1}^n & \cdots &  x_{D}^n
\end{bmatrix}}_{\mathbf{X}}
\begin{bmatrix}
  \beta_1 \\ \beta_2 \\ \vdots \\ \beta_D
\end{bmatrix}
+
\begin{bmatrix}
  \epsilon_1 \\
  \epsilon_2 \\
  \vdots \\
  \epsilon_n
\end{bmatrix}
$$

with $x_1,...,x_d$ corresponding to one sample $\mathbf{x}$
and $x_{d+1},...,x_D$ to
the lower triangular matrix elements of
 $\mathbf{x} \mathbf{x}^T$.
(TODO:this is not correct, but currently I do not
understand the code in quadmodel polyfeat)
This can be solved with ordinary least squares
$\beta = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X} \mathbf{y}$.
For our evaluation we use a form of ridge regression.



The solution vector $\beta$ contains the surrogate model parameters
$(r, \mathbf{r}, \mathbf{R})$. We also assume that
the measurement noise $\epsilon_k$ is zero mean Gaussian
$\epsilon_k \sim N(0, \sigma^2)$ distributed.

Using recursive estimation we
can process each pair of samples and rewards $(\mathbf{x}_k, y_k)$ one
at a time.
$$
 y_k =
 (1 \;  x_1^k \; \cdots
 \; x_d^k  \; x_{d+1}^k \; \cdots \;  x_{D}^k)
\begin{pmatrix}
  \beta_1 \\ \beta_2 \\ \vdots \\ \beta_D
\end{pmatrix} 
+ \epsilon_k
$$

\section{Recursive Least Squares}
Let us now show how we use the recursive least squares approach
to compute the solution vector $\beta$ to the regression problem.

Our approach is a special case of the Kalman Filter equations
\href{KF_update}, as we assume the state transition model to be
an identity matrix.

We want to compute
$$ p(\beta_k | y_{1:k-1}, \mathbf{x}_{1:k-1}) = \text{N}(\beta_k | m_k, P_k) $$

Our prior $m_0$ is choosen such that for the
first surrogate model parameters
we set the quadratic term to an identity matrix $\mathbf{R} = \mathbf{I}$
and the other terms to zero $ \mathbf{r} = \mathbf{0}$ and $ r = 0$.
The covariance matrix is initialized as  $P_0 = \delta \mathbf{I}$ with
the parameter $\delta$ controlling how confident
we are in our prior. By limiting the prediction step of the Kalman Filter
\cref{KF_prediction} to adding model noise we get the basic
version of Recursive Least squares \Cref{RLS:basic} with a drift model for the
parameters to be estimated.

\begin{algorithm}[H]
\SetKw{KwInit}{Initialization:}
\DontPrintSemicolon
\SetAlgoLined

\KwIn{$\{(\mathbf{x}_1,y_1),\cdots,(\mathbf{x}_N,y_N)\}$
  stream of samples and rewards, \\
  $Q$ model noise, $\sigma^2$ measurement noise}
\KwInit{$\mathbf{m}_0$,  $\mathbf{P}_0 = \delta \mathbf{I}$}

\For{$n = 1,...,N$}
{
  $\mathbf{P}_n^- = \mathbf{P}_{n-1} + Q$ \;
  $~$ \;
  \Begin(Update Step)
  {
    $S_n = \phi(\mathbf{x}_n) \mathbf{P}_n^- \phi(\mathbf{x}_n)^T + \sigma^2$ \;
    $\textbf{K}_n = \textbf{P}_{n}^{-} \textbf{H}^T_n S_n^{-1}$ \;
    $\textbf{m}_n = \textbf{m}_{n-1} + \textbf{K}_n [y_n - \textbf{H}_n \textbf{m}_{n-1}]$ \;
    $\textbf{P}_n = \textbf{P}_{n}^{-} - \textbf{K}_n S_n \textbf{K}_n^T $ \;
  }
}
\caption{Recursive Least squares with Drift Model}
\label{RLS:basic}
\end{algorithm}

\section{Data Preprocessing Techniques}
To improve the performance of the algorithm we examined several
data processing techniques. Some key challenges
that arise in the data  are
high range of objective values,
and dealing with sharp bumps in reward from
penalties (e.g collisions).

\subsection{Whitening}
Whitening is common data preprocessing method in statistical analysis
to transform a correlated random vector into an uncorrelated one
\citet{kessy2018optimal}.

We employ whitening to our algorithm, because uncorrelated
random variables often simplify  tracking
the parameters (cite).

\textit{Whitening} is a linear transformation that converts a $d$-dimensional
random vector $\mathbf{x} = (x_1,...,x_d)^T$ with mean
$\text{E}(\mathbf{x}) = \mathbf{\mu} = (\mu_1,...,\mu_d)^T$ and
positive definite $d \times d$ covariance matrix
var$(\mathbf{x}) = \Sigma$ into a new random vector
\begin{align}
  \label{whitening}
 \mathbf{z} = (z_1,...,z_d)^T = \mathbf{W}\mathbf{x}
\end{align}

of the same dimension $d$ and with unit diagonal ``white'' covariance
var$(\mathbf{z}) = \mathbf{I}$. The square $d \times d$
matrix $\mathbf{W}$ is called the whitening matrix.

The whitening transformation defined in Equation \Cref{whitening} requires
the choice of a suitable whitening matrix $W$.
Since $\text{var}(z) = \mathbf{I}$ it follows that
$\mathbf{W}\Sigma \mathbf{W}^T = \mathbf{I}$ and thus
$\\mathbf{W}(\Sigma \mathbf{W}^T\\mathbf{W}) = \mathbf{W}$, which
is fulfilled if $\mathbf{W}$ satisfies the condition
$$ \mathbf{W}^T \mathbf{W} = \Sigma^{-1} $$

This constrain does not uniquely determine the whitening
matrix $\mathbf{W}$, instead given $\Sigma$ there are infinitely many
possible matrices $\mathbf{W}$, because it allows for rotational freedom.

We used \textit{Cholesky whitening} which is based on
Cholesky factorization of the precision matrix
$\mathbf{L}\mathbf{L}^T = \Sigma^{-1}$, which leads to the whitening
matrix:
$$ \mathbf{W}^{\text{Chol}} = \mathbf{L}^T $$.

If the cholesky whitening is unsuccessful we fall back to standardizing
the random vector meaning $\text{var}(\mathbf{z}) = 1$ but the correlations
are not removed. In \cref{fig:whitening} we provide an example on using
whitening for our parameter estimation task.

\begin{figure}[t]
  \centering
  \subfigure[][without whitening]{\input{figures/LS_no_whitening}}
  \subfigure[][in whitened space]{\input{figures/LS_whitening}}
  \subfigure[][unwhitened parameters]{\input{figures/LS_unwhitened}}
  \caption{Example of using LS on 2-dimensional
   Rosenbrock function. The plots show the predicted surrogate model
   parameters $\mathbf{\beta}$. We can see that
   the parameters in whitend space stay in a smaller range 
   making the estimation task easier.}
 \label{fig:whitening}
\end{figure}
  
\subsection{Sample pool}
The original MORE algorithm and other policy search algorithms (REPS) we
use a sample pool.

Theoretically  we would want to avoid using a sample pool and instead use
the information form past samples in the form of the previously predicted
model parameters. But with our algorithm using no sample pool led
to unsatisfactory estimation results.

We increased the model noise for older samples, encoding the fact that
older samples should have a higher covariance, meaning we are less
certain about them. We implemented this by simple adding a constant
amount to the covariance of the RLS equations for each time a sample is
used. For higher dimensional task this is around 5-10 times.
Still the use of a sample pool is theoretically unsound and problematic,
instead a accurate state transition model could be introduced.

\subsubsection{Higher model noise for older samples}
Using the recursive least squares algorithm we had to maintain a sample
pool, because otherwise the resulting estimation was not
satisfactory. From a theoretic perspective this sloppy and ideally we
would refrain from using the sample pool.

We introduced a simple counter for each sample, indicating how many times
the sample has been used.
Then for older samples (higher counter) we increased the model
noise proportionally.
We tried a linear relationship between the counter and
the increased covariance for the model parameters.

$$ new\_cov = constant\_cov\_matrix + w * \text{counter} * \mathbf{I} $$

- TODO: try different weightings: exponential

This improved our results on the rosenbrock test function compared
to the RLS version with constant model noise for all samples.
In each MORE iteration first the oldest samples from the sample pool
are used, meaning the recent samples with lowest model noise are
processed last by RLS.

\subsubsection{No Sample Pool}


\subsection{Normalization}
Original MORE approach uses standard score normalization, doing it
in a batch way at each iteration for all samples in the sample pool.

This is not fit for recursive estimation.
- different types of reward functions difficulties:
- rosenbrock (high range of values)
- sharp spikes, in rewards (punishing term for reaching task)

- simply online calculation of mean and var for normalization is not
a good fit (include plots and results of investigation)

- instead use moving average to calculate mean and var only of window (discard old
  data) 

- describe moving average computation of normalization

\section{MORE with Recursive Surrogate-Modeling}
\begin{algorithm}[H]
\DontPrintSemicolon
\SetAlgoLined
\KwIn{Parameters $\epsilon$ and $\beta$, initial distribution, \\ $K$ number of iterations, $N$ samples per iteration}

\For{$k = 1,...,K$}
{
  \For{$n = 1,...,N$}
p  {
    Draw parameters $\theta_n \sim \pi$\;
    Execute task with $\theta_n$ and receive $R(\theta_n)$\;
  }
  \Begin(Learn quadratic model with Recursive Least Squares)
  {
    \For{$n = 1,...,N$}
    {
      Whitening: $\mathbf{W}\phi(\mathbf{x}_n)$\;
      Optionally: Use normalization\;
      Optionally: Increase model noise for older samples\;      
      compute surrogate model parameters with RLS (\ref{RLS:basic})\;
    }
  }
  Minimize dual function $g(\eta, \omega)$ using Eq.\;
  Update search distribution $\pi$ using \Cref{policy_update}\;
}
\caption{MORE Algorithm with Recursive Surrogate-Modeling}
\end{algorithm}

%\subsection{Moving back to prior}
%One technique we investigated is 
%- to prior calculation (optimal for whitened space)
%
%- set up equation
%
%\subsection{State Transition Model}
%Let us quickly state the full Kalman filter equations, the
%derivation can be found in the appendix (TODO ref)
%
%Prediction step:
%
%Update step:
%
%Introducing an accurate state transition model to incorporate the
%prediction step of the kalman filter would be ideal.
%We tried some simple momentum approaches based on the differences of
%subsequent surrogate model parameters, but those did not yield
%good results.
%Our Recursive least squares algorithm is a special case of these
%equations using an state transition model matrix $\mathbf{A} = \mathbf{I}$.
%