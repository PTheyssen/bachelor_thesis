% !TeX spellcheck = en_US
% !TeX encoding = UTF-8
% !TeX root = ../thesis.tex

\chapter{Recursive Surrogate-Modeling for MORE}
In this chapter we will first motivate the idea of recursively
estimating the surrogate model for the MORE algorithm.
We formulate the surrogate model estimation
as a regression problem and present our solution.
Then we combine our approach with the MORE algorithm
Finally we discuss various data preprocessing techniques.

\section{Surrogate Model Estimation}

\subsection{Motivation}
So far, the surrogate model has been estimated from scratch in each
iteration using samples and corresponding
values of the objective function. 
However, subsequent models are correlated
due to the locality of the data (see figure
(TODO: add Figure of surrogate model)).
This arises from the fact that the KL-divergence is
used to bound the distance between
subsequent search distributions. Therefore recursive estimation
techniques may be able to utilize the information of previous
surrogate models and thus reduce the total amount of samples needed.
Additionally this may also reduce the runtime of the algorithm.

The surrogate model is a of the form
\begin{align}
  \label{surrogate}
  \mathcal{R}(\mathbf{x}) = -\frac{1}{2} \mathbf{x}^T \mathbf{R} \mathbf{x}
  + \mathbf{x}^T \mathbf{r} + r 
\end{align}
With a quadratic term $\mathbf{R} \in \mathbb{R}^{n \times n}$,
a linear term $\mathbf{r} \in \mathbb{R}^{n}$ and a scalar $r$,
here $n$ denotes the dimension of the objective function.
Using a quadratic model is sufficient as the exponential of the
Gaussian is also quadratic in the parameters.
A more complex model could not be exploited by
a Gaussian distribution \citep{abdolmaleki2015model}.


% TODO: include 2D contour line or 3d version of subsequent models

\subsection{Regression Problem}
We formulate the surrogate estimation task as
a regression problem (\ref{eq:regression}) by using
a feature function $\phi(\mathbf{x})$ which returns
a bias term, all linear and all quadratic terms. The dimensionality of
$\phi(\mathbf{x})$ is $D = 1 + d + d(d + 1) / 2$, where $d$
is the dimensionality of the parameter space.
\begin{align}
  \label{eq:regression}
  y = \phi(\mathbf{x}) \beta + \epsilon
\end{align}
% The surrogate model parameters are in $\beta$
% a vector of length $D$ containing first the
% scaler than $d$ linear terms and then $d(d+1) /2$ quadratic terms
% (the lower triangular matrix).
Our data are the samples and corresponding objective function
values $\mathcal{D} = \{(\mathbf{x}_1, y_1),...,(\mathbf{x}_n, y_n)\}$.
To solve the regression problem we set up
the design matrix $\mathbf{X}$ as depicted in \cref{eq:lgs}.
One row $\mathbf{X}_{k}$ first contains a one for the bias term.
Then the next $d$ entries are made up of the corresponding
sample $\mathbf{x}_k$ for the linear term.
The final $d(d + 1) / 2$  entries are
the lower triangular matrix of the product $\mathbf{x}_k \mathbf{x}_k^T$,
which we denote by tril$(\mathbf{x}_k \mathbf{x}_k^T)$.
% Corresponding to the set
% $\text{tri}(\mathbf{x}) = \{x_1^2 , x_1 x_2 , x_1 x_3 ,
% \cdots , x_2^2 , x_2 x_3 \cdots\}$,
% where we multiply each sample entry with itself and all entries with
% higher index. 

For the least squares (LS) approach with $n \geq D$ samples
we get the following system of linear equations. 
\begin{equation}
  \label{eq:lgs}
\begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{bmatrix}
=
\underbrace{
\begin{bmatrix}
  1 & \mathbf{x}_1 & \text{tril}(\mathbf{x}_1 \mathbf{x}_1^T) \\
  1 & \mathbf{x}_2 & \text{tril}(\mathbf{x}_2 \mathbf{x}_2^T) \\
  \vdots & \vdots & \vdots \\
  1 & \mathbf{x}_n & \text{tril}(\mathbf{x}_n \mathbf{x}_n^T) \\
\end{bmatrix}}_{\mathbf{X}}
\begin{bmatrix}
  \beta_1 \\ \beta_2 \\ \vdots \\ \beta_D
\end{bmatrix}
+
\begin{bmatrix}
  \epsilon_1 \\
  \epsilon_2 \\
  \vdots \\
  \epsilon_n
\end{bmatrix}
\end{equation}

This can be solved with ordinary least squares
$\beta = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X} \mathbf{y}$.
For our experiments we use a form of ridge regression \citep{hoerl1975ridge}.


The solution vector $\beta$ contains the surrogate model parameters
$(r, \mathbf{r}, \mathbf{R})$. We also assume that
the measurement noise $\epsilon_k$ is zero mean Gaussian
$\epsilon_k \sim N(0, \sigma^2)$ distributed.


Using recursive estimation we
can process each pair of samples and rewards $(\mathbf{x}_k, y_k)$ one
at a time.
$$
 y_k =
 \big(1 \;  \mathbf{x}_k \; \text{tril}(\mathbf{x}_k \mathbf{x}_k^T) \big)
\begin{pmatrix}
  \beta_1 \\ \beta_2 \\ \vdots \\ \beta_D
\end{pmatrix} 
+ \epsilon_k
$$

\section{Recursive Least Squares}
Let us now show how we use the recursive least squares approach
to compute the solution vector $\beta$ to the regression problem.

We did not find a good state transition model for our problem of
estimating the parameters of the quadratic model of the objective function.
We tried some momentum based approaches during our evaluation but
did not receive any promising results. Therefore we limit our approach to
adding model noise in the prediction step and
using an Identity matrix as the state transition matrix $\mathbf{A}$.

We want to compute
$$ p(\beta_k | y_{1:k-1}, \mathbf{x}_{1:k-1}) = \text{N}(\beta_k | m_k, P_k) $$

Our prior $m_0$ is chosen such that for the
first surrogate model parameters
we set the quadratic term to an identity matrix $\mathbf{R} = \mathbf{I}$
and the other terms to zero $ \mathbf{r} = \mathbf{0}$ and $ r = 0$.
The covariance matrix is initialized as  $P_0 = \delta \mathbf{I}$ with
the parameter $\delta$ controlling how confident
we are in our prior. By limiting the prediction step of the Kalman Filter
\cref{KF_prediction} to adding model noise we get the basic
version of Recursive Least squares \Cref{RLS:basic} with a drift model for the
parameters to be estimated.

\begin{algorithm}[H]
\SetKw{KwInit}{Initialization:}
\DontPrintSemicolon
\SetAlgoLined

\KwIn{$\{(\mathbf{x}_1,y_1),\cdots,(\mathbf{x}_N,y_N)\}$
  stream of samples and rewards, \\
  $Q$ model noise, $\sigma^2$ measurement noise}
\KwInit{$\mathbf{m}_0$,  $\mathbf{P}_0 = \delta \mathbf{I}$}

\For{$n = 1,...,N$}
{
  $\mathbf{P}_n^- = \mathbf{P}_{n-1} + Q$ \;
  $~$ \;
  \Begin(Update Step)
  {
    $S_n = \phi(\mathbf{x}_n) \mathbf{P}_n^- \phi(\mathbf{x}_n)^T + \sigma^2$ \;
    $\textbf{K}_n = \textbf{P}_{n}^{-} \textbf{H}^T_n S_n^{-1}$ \;
    $\textbf{m}_n = \textbf{m}_{n-1} + \textbf{K}_n [y_n - \textbf{H}_n \textbf{m}_{n-1}]$ \;
    $\textbf{P}_n = \textbf{P}_{n}^{-} - \textbf{K}_n S_n \textbf{K}_n^T $ \;
  }
}
\caption{Recursive Least squares with Drift Model}
\label{RLS:basic}
\end{algorithm}


\section{Data Preprocessing Techniques}
To improve the performance of the algorithm we examined several
data processing techniques. Some key challenges
that arise in the data  are
high range of objective values,
and dealing with sharp bumps in reward from
penalties (e.g collisions).

\subsection{Whitening}
Whitening is a common data preprocessing method in statistical analysis
to transform a correlated random vector into an uncorrelated one
\citep{kessy2018optimal}.
We employ whitening to our algorithm, to reduce the complexity of the
parameters to be estimated. 

\textit{Whitening} is a linear transformation that converts a $d$-dimensional
random vector $\mathbf{x} = (x_1,...,x_d)^T$ with mean
$\text{E}(\mathbf{x}) = \mathbf{\mu} = (\mu_1,...,\mu_d)^T$ and
positive definite $d \times d$ covariance matrix
var$(\mathbf{x}) = \Sigma$ into a new random vector
\begin{align}
  \label{whitening}
 \mathbf{z} = (z_1,...,z_d)^T = \mathbf{W}\mathbf{x}
\end{align}

of the same dimension $d$ and with unit diagonal ``white'' covariance
var$(\mathbf{z}) = \mathbf{I}$. The square $d \times d$
matrix $\mathbf{W}$ is called the whitening matrix.

The whitening transformation defined in Equation \Cref{whitening} requires
the choice of a suitable whitening matrix $W$.
Since $\text{var}(z) = \mathbf{I}$ it follows that
$\mathbf{W}\Sigma \mathbf{W}^T = \mathbf{I}$ and thus
$\mathbf{W}(\Sigma \mathbf{W}^T\mathbf{W}) = \mathbf{W}$, which
is fulfilled if $\mathbf{W}$ satisfies the condition
$$ \mathbf{W}^T \mathbf{W} = \Sigma^{-1} $$
This constrain does not uniquely determine the whitening
matrix $\mathbf{W}$, instead given $\Sigma$ there are infinitely many
possible matrices $\mathbf{W}$, because it allows for rotational freedom.

We used \textit{Cholesky whitening} which is based on
Cholesky factorization of the precision matrix
$\mathbf{L}\mathbf{L}^T = \Sigma^{-1}$, which leads to the whitening
matrix $\mathbf{W}^{\text{Chol}} = \mathbf{L}^T$.
If the cholesky whitening is unsuccessful due to numerical problems,
we instead standardize
the random vector meaning $\text{var}(\mathbf{z}) = 1$ but the correlations
are not removed. In \cref{fig:whitening} we provide an example on using
whitening for our parameter estimation task.

\begin{figure}[t]
  \centering
  \subfigure[][without whitening]{\input{figures/LS_no_whitening}}
  \subfigure[][in whitened space]{\input{figures/LS_whitening}}
  \subfigure[][unwhitened parameters]{\input{figures/LS_unwhitened}}
  \caption{Example of using LS on 2-dimensional
   Rosenbrock function. The plots show the predicted surrogate model
   parameters $\mathbf{\beta}$. We can see that
   the parameters in whitened space stay in a smaller range 
   making the estimation task easier.}
 \label{fig:whitening}
\end{figure}
  
\subsection{Sample pool}
The original MORE algorithm and other policy search algorithms (REPS)
use a sample pool.
In our approach  we want to avoid using a sample pool and instead use
the information form past samples in the form of the previously predicted
model parameters.

- Still in our experiments the performance better with pool

- talk about doing warm start with no pool

- without pool simply performs worse (but still possible --> maybe
better model noise, prediction parameters)

Idea of increasing the model noise for older samples, encoding the fact that
older samples should have a higher covariance, meaning we are less
certain about them. We implemented this by simple adding a constant
amount to the covariance of the RLS equations for each time a sample is
used. For higher dimensional task this is around 5-10 times.
Still the use of a sample pool is theoretically unsound and problematic,
instead a accurate state transition model could be introduced.
Using the recursive least squares algorithm we had to maintain a sample
pool, because otherwise the resulting estimation was not
satisfactory. From a theoretic perspective this sloppy and ideally we
would refrain from using the sample pool.

We introduced a simple counter for each sample, indicating how many times
the sample has been used.
Then for older samples (higher counter) we increased the model
noise proportionally.
We tried a linear relationship between the counter and
the increased covariance for the model parameters.

$$ new\_cov = constant\_cov\_matrix + w * \text{counter} * \mathbf{I} $$

- TODO: try different weightings: exponential

This improved our results on the rosenbrock test function compared
to the RLS version with constant model noise for all samples.
In each MORE iteration first the oldest samples from the sample pool
are used, meaning the recent samples with lowest model noise are
processed last by RLS.


\subsection{Normalization}
Original MORE approach uses standard score normalization, doing it
in a batch way at each iteration for all samples in the sample pool.

different ways of normalization:

- exponential weighting (search for citation)

- standard score

\section{MORE with Recursive Surrogate-Modeling}
\begin{algorithm}[H]

\DontPrintSemicolon
\SetAlgoLined
\KwIn{Parameters $\epsilon$ and $\beta$, \\ $K$ number of iterations, $N$ samples per iteration}
\Begin{Initialization:}
{
  Initialize search distribution $\pi$
  Initialize RLS$(\mathbf{m}_0, \mathbf{P}_0)$
}
\For{$k = 1,...,K$}
{
  \For{$n = 1,...,N$}
  {
    Draw parameters $\theta_n \sim \pi$\;
    Execute task with $\theta_n$ and receive $R(\theta_n)$\;
  }
  \Begin(Estimate quadratic model)
  {
    \For{$n = 1,...,N$}
    {
      Whitening: $\mathbf{W}\phi(\mathbf{\theta}_n)$\;
      Optionally: Use normalization\;
      Optionally: Increase model noise for older samples\;      
      compute surrogate model parameters with RLS  \cref{RLS:basic}\;
    }
  }
  Solve  $\text{argmin}_{\eta >0, \omega > 0} \, g(\eta, \omega)$
  using \Cref{eq:dual} \;
  Update search distribution $\pi$ using \Cref{policy_update}\;
}
\caption{MORE Algorithm with Recursive Surrogate-Modeling}
\end{algorithm}



%\subsection{Moving back to prior}
%One technique we investigated is 
%- to prior calculation (optimal for whitened space)
%
%- set up equation
%
%\subsection{State Transition Model}
%Let us quickly state the full Kalman filter equations, the
%derivation can be found in the appendix (TODO ref)
%
%Prediction step:
%
%Update step:
%
%Introducing an accurate state transition model to incorporate the
%prediction step of the kalman filter would be ideal.
%We tried some simple momentum approaches based on the differences of
%subsequent surrogate model parameters, but those did not yield
%good results.
%Our Recursive least squares algorithm is a special case of these
%equations using an state transition model matrix $\mathbf{A} = \mathbf{I}$.
%