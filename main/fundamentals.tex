% !TeX spellcheck = en_US
% !TeX encoding = UTF-8
% !TeX root = ../thesis.tex

\chapter{Fundamentals}
This chapter introduces basic concepts used throughout this thesis.
First basics of reinforcement learning and the problem of robot learning.
Next we discuss policy search, a sub-field of reinforcement learning,
as one method to solve the robot learning problem.
The Kullback-Leibler divergence (KL), as an important information theoretic
distance metric between probability distributions.
Having discussed the underlying basics we can then
introduce the MORE Algorithm.
Finally we will look at Filtering from a Bayesian Estimation viewpoint and
review the Kalman Filter for parameter estimation.

\section{Reinforcement Learning}
Reinforcement Learning is a subfield of Machine Learning concerned with agents
learning to interact with their environment.
This is done through exploration and trial-and-error, trying to discover
cause and effect relationship between actions.
Compared to supervised learning and unsupervised learning it
more closely resembles the way humans learn. \\
As \citet{sutton2018reinforcement} puts it,
the term reinforcement learning relates to a class of problems,
solution methods and the field that studies these problems and solutions.
Some famous examples include playing games like Go
\citet{silver2016mastering} and Atari games \citet{mnih2013playing}
Generally RL is applicable to a large range of problems.
Whereas in supervised learning the best action is presented to the system,
the agent in a reinforcement learning setting receives an occasional
reward (or punishment).
To gain information about the rewards the agent needs
to explore previously unused actions.
Dare to try new things or keep performing safe
well-known actions, this is the \textit{exploration-exploitation tradeoff}.
The agent should exploit actions he knows that
give decent reward, but he first
has to try different things to learn about these actions,
and then he has to progressively focus in on them.
The reward signal is only given occasionally, hence the amount of information
the agent receives is minimal compared to
supervised and unsupervised learning approaches.

We model the agent and its environment as a state $s \in S$. The agent
may perform action $ a \in A$ which can be either discrete or continuous.
For every action step the agent receives a Reward $R$,
which is a scalar value.
The overarching goal is to find a mapping from states to actions,
called policy $\pi$, that picks actions in a way that
the reward is maximized.
We can distinguish an \textit{episodic} setting
from an on-going task.
In the episodic setting
the task is restarted after
the end of an episode and the goal is
to maximize the total reward per episode.
In on-going tasks the goal
is to simply achieve high average reward over
the whole life-time or one can use a formulation
with a discounted return (weighting the
future and past differently).

The classical approach to formalizing problems in RL is through
Markov Decision Processes (MDPs).
MDPs are a mathematical
framework for decision making in deterministic and stochastic environments.
MDPs focus on only three aspects
- sensation, action and goal, which are central
for reinforcement learning problems.
MDPs satisfy the Markov property (cite), which state that ``the future is independent
of the past given the present''. In our case this means the next state $s'$ and the reward
only depend on the previous state $s$ and action $a$ \citet{sutton1992reinforcement}.
A MDP can be formally defined as a tuple $(S, A, P, r)$:

\begin{itemize}
\item a set of states $s \in S$ that describe the environment.
\item a set of actions $a \in A$ that can be performed by the agent in the environment
\item a transition function $P(s_{t+1} | s_t, a_t)$ that gives the probability of a new
  state $s_{t+1}$ after an action $a_t$ has been taken in state $s_t$
\item a reward function $r(s_t, a_t)$ that specifies the immediate reward after taking action
  $a_t$ in state $s_t$
\end{itemize}

The Markov property can be expressed as
$$ P(s_{t+1} | s_t, a_t, s_{t-1}, a_{t-1}\cdots) = P(s_{t+1} | s_t, a_t)$$
This recapitulates the notion of state - a state is a sufficient statistic
for predicting the future, rendering previous observations irrelevant.
In robotics we may only find some approximate notion of state. \\
Generally the goal is to find an optimal policy $\pi*$,
a mapping from states to actions that
maximizing the expected return $J$.
Optimal behavior can be modeled in
different ways, resulting in different
definitions for expected return. \\
Finite-horizon model, for horizon $H$ meaning the next $H$ timesteps:
$$ J = E \left\{\sum^H_{t=0} R_t \right\} $$

Discounted reward, discounting future rewards by a discount factor $\gamma$
(with $0 \leq gamma < 1)$:
$$ J = E \left\{\sum^{\infty}_{t=0} \gamma^t R_t \right\} $$

Reinforcement learning can also be seen as
general case of optimal control as in \citet{sutton1992reinforcement},
whereas optimal control assumes perfect knowledge, RL uses approximations
and data-driven techniques.

- write about Value learning, and shortly name traditional methods (multiarmbandit,
DP, Temporal difference learning)


\section{Robot Learning}
The sub-field where reinforcement learning and machine learning
intersect with robotics is called \textit{Robot Learning}. It aims to bridge
the gap between programmed robots,
with fine tuned controllers  and fully autonomous robots.
As proposed in \citet{deisenroth2013survey}, robot control can be modeled as
a reinforcement learning problem. \\
The state space $\mathbf{x}$ in robotic tasks is high dimensional and made up of
the internal state of the robot (e.g., joint position, body position/ orientation)
and external state (e.g. object locations, lighting). The true state is
not observable and also not noise free. 
The Robot chooses its next motor control $u$ according to a policy $\pi$.
This policy $\pi$ may
be deterministic $a = \pi(s)$ or stochastic $a \sim \pi(s,a) = P(a | s)$.
The motor commands $u$ alter the state according to the probabilistic
transition function $p(\mathbf{x}_{t+1} | x_t, u_t)$. This transition function
is not known, in model-based policy search this function is learned from data and
used to improve the policy.
Collectively the states and actions of the robot form a
\textit{trajectory} $\tau = (x_0, u_0, x_1, u_1,...)$ which is also called
a \textit{rollout} or a \textit{path}.
There has to be a numeric scoring system assessing the quality
of the robots trajectory and returning a reward signal $R(\tau)$.
For \textit{episodic learning tasks} the task ends after a given number $T$ of time
steps. Then the accumulated reward $R(\tau)$ for a trajectory is given by

$$ R(\tau) = r_T(x_T) + \sum^{T-1}_{t=0} r_t(x_t,u_t) $$

where $r_t$ is an instantaneous reward function (e.g. a punishment for energy consumed)
and $r_T$ the final reward, when performing a reaching task this may take the
form of a quadratic punishment term for deviation from the goal posture.

If we consider an infinite-horizon for an on-going task we get
$$ R(\tau) = \sum^{\infty}_{t=0} \gamma^{\; t} r(x_t, u_t) $$
where $\gamma \in [0,1)$ is a discount factor that discounts rewards further in the future.

Many tasks in robotics can thus be formulated as choosing a optimal control
policy $\pi^*$ that maximized the expected accumulated reward

$$ J_{\pi} = \mathbb{E}[R(\tau) | \pi] = \int R(\tau) p_{\pi}(\tau) d\tau $$
where $R(\tau)$ defines the objectives of the task, and $p_{\pi}(\tau)$ is the
distribution over trajectories $\tau$.

Formulating robotic tasks in this way allows us to use methods from
reinforcement learning. 

\subsection{Challenges}
Reinforcement learning in general is a difficult problem, it can be harder
to make RL work compared to supervised or unsupervised learning. One reason
for this is that the reward signal may only be given occasionally and
even then it may be unclear which of
the agents actions were responsible for a certain
reward signal. 

Robotics is different compared to other fields where Reinforcement Learning
is used extensively. First of all the states and actions of the robots in the
real world are inherently continuous requiring us to
deal with the resolution.
In addition the state space can have a high dimensionality and
working with real world systems on real hardware is costly and
makes manual interventions necessary.
Robots require algorithms to run in real-time and working with
real sensors further introduces discrepancy between sensing and
execution.

Most traditional methods from RL like
TD-learning \cite{sutton2018reinforcement}
have been unfit for these particular requirements of robotic tasks.
Stressing the importance of robotics as a testing ground for RL that
demands new developments and innovative research.
We will now discuss some problems encountered when applying
RL to robotics, this treatment does only focus on a certain
set of challenges and is not exhaustive.

\subsubsection{Curse of Dimensionality}
The term ``Curse of Dimensionality'' was coined by \citet{Bellman:1957} when
he explored optimal control in higher dimensions and encountered
an exponential explosion of the states and actions.
For example, for the evaluation of our algorithm we run a simulation of a
simple planar reaching task
with a 5 link robot arm using DMPs
for policy representation and get a 25 state dimension.
Especially modern anthropomorphic robots tend to have many degrees of
freedom.

The agent in RL generally needs to collect data throughout the entire
state-space to ensure global optimization, for robotics dealing with
three dimensional space makes this very difficult.
To alleviate this problem  a widespread idea is to use expert demonstration
to get a good initialization for the agents policy, this
eliminates the need to search the entire search space, instead the
agent can focus on locally optimizing the initialized policy.
This concept of imitation learning focuses on the problem of ``learning
from demonstrations'' which plays an important role
for robotics \citet{Osaetal18}. Imitation learning
will enable domain experts to teach motions and
skills without special knowledge about robotics, which will be crucial
when robots start making their way from factories into everyday life.

\subsubsection{Curse of Real-world Samples}
Robot hardware is expensive and suffers from wear and tear, making
costly maintenance necessary. Hence safe methods for real robots
should avoid big jumps in policy updates, as such sudden changes may
result in unpredictable movements and consequently damage the robot.
This problem is commonly referred to as \textit{safe exploration}.
Whereas in traditional reinforcement learning
safe exploration does not receive much attention,
it has become a key issue for robot learning \citet{schneider1997exploiting}.
Different external factors like temperature may change the robot dynamics and
additional uncertainty from the sensors makes it difficult to reproduce and
compare results.

Most task also require a ``human in the loop'' who either supervises the robot
or resets the setup after one episode. Even if this can be avoided the learning
speed simply cannot be sped up.
For a single robot the training time has a natural limit
and in total only relatively few executions can be completed.
One method for gathering more data is ``collective robot learning'' described in
\citet{kehoe2015survey}. The idea is for multiple robots to
share their data on trajectories, policies and outcomes.Currently this seems only
viable for large corporations with significant capital.
Thus in robot learning the constraint of using only a small number of trials
is given more weight than limiting memory consumption or computational complexity.
Thus sample efficient algorithms are essential.
Generally the state represented by sensors slightly lags behind the real
state due to processing and communication delays. This is in stark contrast to
most other reinforcement learning algorithms, which assume actions to take effect
instantaneously.

\subsubsection{Curse of Goal Specification}
Defining a good reward function in robot reinforcement learning is
difficult and often needs a lot of domain knowledge and expertise.
There are certain trade-offs to keep in mind, for example performing
a powerful swing for a hitting task may yield high reward but may damage
or shorten the life-time the robot.
Reinforcement learning algorithms also tend to solve tasks
in unintended ways exploiting the reward function in some unforeseen way.

Inverse reinforcement learning \citet{russell1998learning}
provides an alternative  to specifying the reward function manually.
The goal of inverse reinforcement learning is to recover the unknown
reward function from the expert's trajectories.

Still learning algorithms are rarely employed on robots
for real daily usage. Also most
algorithms are over fitted to a particular robot architecture
and do not generalize to other robots easily.
A large number of different methods exist, but
in general there is no clear recipe for robot learning.
Showing that robot learning still has a lot open problems,
and will continue to grow as a research field.
Even minor flaws or errors in the employed method can completely prevent
success of learning.
Nonetheless appropriately chosen algorithms and rewards functions
already achieve promising results.


\subsection{Policy search}
Many traditional methods in RL try to estimate the
expected long-term reward of a policy for each state $\mathbf{x}$
and time step $t$, this leads to formulation of the value function $V^{\pi}_t(\mathbf{x})$.
With the value function we can asses the quality of executing action
$\mathbf{u}$ in state $\mathbf{x}$. This assessment is used
to directly compute the policy by action selection or to update
the policy $\pi$. As value function methods struggle with the
challenges in reinforcement learning, the main approach
for robotics has become policy search.
Policy search methods opposed to value-based methods
use parameterized policies $\pi_{\theta}$ and search
directly in the parameter space $\Theta$. This allows using RL with
high-dimensional continuous action spaces encountered
in robotics by reducing the search space of possible policies.
Policy search further allows the usage of predefined
task-appropriate policy representations like Dynamic
Movement Primitives \citet{schaal2005learning}, as well
as easily integrating imitation learning
for policy initialization.

Generally we can divide policy search into model-free and model-based and
differentiate whether stochastic or deterministic trajectories are used.
Model-free policy search uses trajectories from the robot directly
for updating the policy. Model-based methods use the data
from the robot to learn a model of the robot. This model is then used
to generate trajectories that are used for policy updates.

- include figure for policy search taxonomy
  (recreated from \citet{deisenroth2013survey})
  also include examples for each type (REINFORCE, REPS, PILCO,...)

The most important concept is computing the policy updates.
Both model-free and model-based policy search use policy gradients (PG),
expectation-maximization (EM)-based updates, or
information-theoretic insights (Inf.Th.).

In this thesis we will focus on model free policy search methods
where the trajectories are generated by ``sampling'' from
the robot.
Due to their simplicity and ease of use of these algorithms
are used more frequently than model-based policy search methods.
Specifically we will focus on stochastic
search algorithms, which are general black-box optimizers.
They are used in a wide range of fields like operations research,
machine learning and also policy search.
Since these algorithms do not use any knowledge about the
objective function it is straightforward to
apply them to policy search in the episode-based formulation.

Using stochastic search algorithms we keep an upper-level policy
$\pi_{omega}(\theta)$ which selects the parameters of the
actual control policy $\pi_{\theta}(\mathbf{u} | \mathbf{x})$ of the robot.
With this instead of directly finding the parameters $\theta$ of the
lower-level policy we want to find the parameter vector $\omega$ which
defines a distribution over $\theta$. We can then use this
search distribution to directly explore the parameter space.


\section{Kullback-Leibler (KL) Divergence}
Many algorithms in policy search rely on the Kullback-Leibler Divergence, also
known as the relative entropy, for controlling
the difference between the the old and updated policy.
Working with real robots additionally requires to perform safe exploration, big
exploration steps, may result in damaging the hardware.
Specifically, it measures the Shannon entropy of one distribution relative to the
other.

$$ KL(p || q) = \int p(\theta) \text{log} \frac{p(\theta)}{q(\theta)} d \theta $$

Note that in general the relative entropy is not symmetric under interchange of the
distributions $p$ and $q$. In general $KL(p || q) \neq KL(q || p) $, therefore
in a mathematically sense it is not strictly a distance.

The KL-bound has been used successfully for many algorithms (cite REPS, Natural gradient)
It is widely used in Machine Learning algorithms. (cite..)


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Constraint Optimization}
%- look at mechano informatik slides
%
%In constraint optimization we want to maximize a function subject to constraints on
%the variables. Generally we have the following problem formulation
%
%\begin{align}
%  \min_{x\in \mathbf{R}^N} f(x)  \text{subject to}
%  \begin{cases}
%    c_i(x) = 0 \\
%    c_i(x) \geq 0
%  \end{cases}
%\end{align}
%
%
%Introducing the Lagragian function we get
%\begin{align}
% \mathcal{L}(\pi, \eta, \omega) = 
%\int \pi(\theta) \mathbf{R}_{\theta} d\theta \; + \; 
%\eta  \left(\epsilon - \int \pi(\theta) \text{ log}
% \frac{\pi(\theta)}{q(\theta)} d\theta\right)
% - \; \omega \left(\beta + \int \pi(\theta) \text{ log}(\pi(\theta)) d\theta\right)
%\end{align}
%
%We can now construct an alternative problem the dual function, which is easier to
%solve than the primal problem. We optimize for the Lagragian multipliers.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{MORE Algorithm}
The MORE algorithm is a stochastic search algorithm. It can be used for
model-free policy search. The key idea is using information-theoretic policy updates
by bounding the relative entropy (Kullback Leibler divergence) between two subsequent
policies. We approximate the objective function with a quadratic surrogate model hence
we can satisfy the KL-bound in closed form.


- TODO: include 3D figure of more algorithm

\subsection{MORE Framework}
The MORE algorithm maintains a search distribution (policy) over the parameter space
of the objective function. We can use the method of constraint optimization
to formulate an optimization problem for obtaining a new search distribution that
maximizes the expected objective value while upper-bounding the KL-divergence and
lower-bounding the entropy of the distribution.

\begin{align}
 \max_{\pi} \int \pi(\theta) \mathbf{R}_{\theta} d\theta, \\
\; \text{ s.t. KL}(\pi(\theta)||q(\theta)) \leq \epsilon, \\
\quad H(\pi) \geq \beta, \\
\quad 1 = \int \pi(\theta) d\theta
\end{align}

Forming the Lagrangian we get
\begin{align} \mathcal{L}(\pi, \eta, \omega) = 
\int \pi(\theta) \mathcal{R}_{\theta} d\theta \; + \; 
\eta  \left(\epsilon - \int \pi(\theta) \text{ log}
 \frac{\pi(\theta)}{q(\theta)} d\theta\right)
 - \; \omega \left(\beta + \int \pi(\theta) \text{ log}(\pi(\theta)) d\theta\right)
\end{align}

The solution is
\begin{align}
\pi(\theta) \propto q(\theta)^{\eta/(\eta+\omega)} 
\text{exp}\left(\frac{\mathcal{R}_\theta}{\eta + \omega}\right)
\end{align}
it depends on the quadratic and linear term of the surrogate model and the
Lagrangian multipliers. These can be obtained by minimizing the dual problem:

\begin{align}
  g(\eta,\omega) = \eta\epsilon - \omega\beta + (\eta - \omega) \text{log}
\left(\int q(\theta)^{\frac{\eta}{\eta + \omega}}
  \text{exp}\left(\frac{\mathcal{R}_\theta}{\eta + \omega}\right) d\theta \right)
\end{align}
We get the closed form solution for the equation
(see full derivation in Appendix).

With these equations we can formulate an iterative algorithm for updating the policy.

\subsubsection{Entropy constraint}
The bound $\beta$ is defined such that relative difference between the entropy
of the policy $H(\pi)$ and a minimum exploration policy $H(\pi_0)$ is decreased
for a certain percentage:
$$ H(\pi) - H(\pi_0) \geq \gamma (H(q) - H(\pi_0))
\rightarrow \beta = \gamma (H(q) - H(\pi_0) + H(\pi_0) $$

\subsection{Surrogate Model}
The key idea of the MORE algorithm is using a surrogate model of the objective function
for satisfying the bound set by the optimization problem. It has proven to be
sufficient to use a quadratic model, since the exponent of Gaussian distribution
is also quadratic, it is not possible to exploit the information of a more
complex surrogate model. This concept of using a surrogate model has been used
in numerical optimization (cite form book numerical optimization)

The original approach of MORE is based on weighted Bayesian linear model with
dimensionality reduction.

\subsection{Analytic Solution for Dual-Function and Policy}
Assuming we are given a quadratic surrogate model
$$ R_\theta \approx \theta^T R \theta + \theta^T r + r_0 $$
we can now solve the dual function in closed form.

$$ g(\eta, \omega) = \eta \epsilon - \beta \omega
+ \frac{1}{2} \left(\mathbf{f}^T \mathbf{F} \mathbf{f} - \eta \mathbf{b}^T \mathbf{Q}^{-1}
\mathbf{b} - \eta log |2\pi \mathbf{Q}| + (\eta + \omega) log |2\pi (\eta + \omega)
\mathbf{F}| \right) $$

Assuming $\pi$ is Gaussian we get the solution, and can update the mean and covariance matrix
of the search distribution. 
\begin{align} \label{policy_update}
  \pi_{t+1} &= \mathcal{N}(\mu_{t+1}, \Sigma_{t+1}) \\
  \mu_{t+1} &= (\eta \Sigma_{t}^{-1}\mu_t + r) / (\eta + \omega) \\
  \Sigma_{t+1} &= (\eta \Sigma_t^{-1} + R) / (\eta + \omega)
\end{align}

With these equations we can iteratively update our search distribution.
It is important to note that MORE can be used as a stochastic search algorithm,
a ``black box solver'' because only the samples and objective function value
is needed to update the search distribution.
In Robotics it can be used to learn motor tasks by using Dynamic Movement Primitives
(DMPs) as a policy representation.

\section{Bayesian Filtering}

TODO:  rewrite, because currently this part to closely resembles \citet{sarkka2013bayesian}.

Optimal filtering is concerned with estimating the state of a time-varying system
which is only partially observed through noisy measurements.
\textit{Bayesian Filtering} refers to the Bayesian way of formulating optimal
filtering.

In general the term ``Bayesian'' refers to inference methods that represent
``degrees of certainty'' using probability theory, and basically use \textbf{Bayes' rule}
to update the degree of certainty given data.

Filtering methods are important in robotics for handling the uncertainty in the
environment. Sensing and modeling the state of the robot itself.
Methods like the Kalman Filter are extensively used since we are dealing with noisy sensors.

Also the data is very limited and when the robots needs to make
decisions it can only work with small amounts of data, therefore
therefore Bayesian methods are a great fit and are widely used (cite..)

In general we solve linear Gaussian and non-linear/non-Gaussian state space
models with the Bayesian filtering equations.

- optimal recursive estimators first presented for linear systems (simplicity)

- most natural optimality criterion is the least squares optimality,
minimum mean squared error

- statistical inversion problem: estimate hidden states from observed
measurements in Bayesian sense we want to compute joint posterior distribution.
Straightforward application of Bayes rule

- Bayesian viewpoint sees probabilities as degree of belief.

- Bayes rule can be used to update our information about a quantity

- Bayesian probability building blocks

\subsection{Building Blocks of Bayesian models}
In general all Bayesian models have some basic components to them.

\subsubsection{prior distribution}
Encodes the information on parameter $\theta$ before seeing any
observations. When we are uncertain about our prior information
we can choose a high variance of the distribution or use a
non-informative prior (which imposes the minimal amount of structure
on the data).
$$ p(\theta) = \text{information on parameter } \theta
\text{before seeing any observations} $$

\subsubsection{measurement model}
Models the relationship between true parameters and the measurements.
$$ p(y | \theta) = \text{distribution of observation } y
\text{given the parameters} \theta $$

\subsubsection{posterior distribution}
The conditional distribution of the parameters given the observations.
It represents the updated belief about the parameters
after obtaining the measurements. It can be computed by using Bayes' rule.

$$ p(\theta | y) = \frac{p(y | \theta) p(\theta)}{p(y)}
\propto p(y | \theta) p(\theta) $$

The normalization constant can be computed by integrating $\theta$ out.

$$ p(y) = \int p(y | \theta) p(\theta) d\theta $$

-TODO: create graphic (2d or 3d) for merging prior with observation to get
    posterior (show with multivariate Gaussian distribution)

\subsection{Gaussian probability function}
A random variable $x \in \mathbb{R}^n$ has a Gaussian distribution with mean
$m \in \mathbb{R}^n$ and covariance $P \in \mathbb{R}^{n\times n}$ if its
probability density has the form
$$ \text{N}(x | m, P) = \frac{1}{(2\pi)^{n / 2} |P|^{1/2}}
\text{exp} \left( -\frac{1}{2} (x - m)^T P^{-1} (x-m) \right) $$
where $|P|$ is the determinant of the matrix $P$.
    
\subsection{Least squares}
To grasp the Bayesian Filtering viewpoint we will now look
at a simple regression problem from a Bayesian perspective
and derive the least squares solution.

We consider a simple linear regression problem,
$$ y_k = \theta_1 + \theta_2 x_k + \epsilon_k $$

note that the response $y_k$ is only dependent on one regressor $x_k$.

Writing this in probabilistic terms we get:
\begin{align}
  p(y_k | \theta) &= \text{N}(y_k | H_k \theta, \sigma^2) \\
  p(\theta) &= \text{N}(\theta | \text{m}_0, \text{P}_0)
\end{align}

Here $H_k = (1 x_k)$ and N$(\dot)$ is the Gaussian probability density function.
The batch solution can then be easily obtained by application of Bayes'rule

$$ p(\theta | y_{1:T})  \propto p(\theta) \prod^T_{k=1} p (y_k | \theta) $$
$$ = N(\theta | m_0, P_0) \prod^T_{k=1} N(y_k | \textbf{H}_k \theta, \sigma^2) $$

Because the prior and likelihood are Gaussian, the \textit{posterior distribution}
will also be Gaussian
$$ p(\theta | y_{1:T}) = \text{N}(\theta | m_t, P_t) $$

Mean and covariance obtained by completing the quadratic form in the exponent:
- TODO: Do calculation in Appendix, (use canonical form)

\begin{align}
  \mathbf{m}_T &= \left[ \mathbf{P}^{-1}_0 + \frac{1}{\sigma^2} \mathbf{H}^T \mathbf{H}
                 \right]^{-1} \left[\frac{1}{\sigma^2} \mathbf{H}^T \mathbf{y} +
  \mathbf{P}^{-1}_0 \mathbf{m}_0 \right] \\
  \mathbf{P}_T &= \left[\mathbf{P}_0^{-1} + \frac{1}{\sigma^2} \mathbf{H}^T \mathbf{H}
                 \right]^{-1}
\end{align}

This gives us the batch least squares solution.

- TODO talk more about how to why this is ordinary least squares?

\subsection{Recursive least squares} \label{RLS}
With the Bayesian formulation we can now  introduce a recursive
solution to the regression problem, resulting in a recursive least squares
algorithm.

For this we have to assume the next step only
depends on the previous state, (markovian assumption).
The main idea is using the previous posterior as the prior in the estimation
step.
The measurements are assumed to be conditionally independent.

To get the \textit{recursive Bayesian solution} we proceed as follows:
In the beginning of estimation all the information about the parameter $\theta$
are contained in the prior distribution $p(\theta)$.
The measurements are obtained on at a time, to
obtain the next posterior distribution we use the measurement and the information
from the previous posterior distribution as the prior.

\begin{align*}
  p(\theta | y_{1}) &= \frac{1}{Z_1} p(y_1 | \theta) p(\theta) \\
  p(\theta | y_{1:2}) &= \frac{1}{Z_2} p(y_2 | \theta) p(\theta | y_1) \\
                    &\vdots \\
  p(\theta | y_{1:T}) &= \frac{1}{Z_T} p(y_T | \theta) p(\theta | y_{1:T-1})
\end{align*}
Normalization term: $Z_k = \int p(\theta) p(y_k | \theta) d\theta$ % normalize likelihood 


Using this approach to solve our simple linear regression model we get
\begin{align}
  p(\theta | y_{1:k}) &\propto p(y_k | \theta) p(\theta | y_{1:k-1}) \\
                      &\propto \text{N}(\theta | \mathbf{m}_k, \mathbf{P}_k)
\end{align}
from which we get:
\begin{align}
  \mathbf{m}_k &= \left[ \mathbf{P}^{-1}_{k-1}
                 + \frac{1}{\sigma^2} \mathbf{H}^T_k \mathbf{H}_k \right]^{-1}
                 \left[\frac{1}{\sigma^2} \mathbf{H}^T_k \mathbf{y}_k +
                 \mathbf{P}^{-1}_{k-1} \mathbf{m}_{k-1} \right] \\
  \mathbf{P}_k &= \left[\mathbf{P}_{k-1}^{-1}
                 + \frac{1}{\sigma^2} \mathbf{H}^T_k \mathbf{H}_k \right]^{-1}
\end{align}

TODO: do calculation with matrix inversion lemma (in appendix?)

By using the \textit{matrix inversion lemma} the covariance calculation can be
written as
$$ P_k = P_{k-1} - P_{k-1} H_k^T [H_k P_{k-1} H_k^T + \sigma^2]^{-1} H_k P_{k-1} $$

By introducing temporary variables $S_k$ and $\mathbf{K}_k$ the calculation of
the mean and covariance can be written in the form
\begin{align*}
     S_k &= \textbf{H}_k \textbf{P}_{k-1} \textbf{H}^T_k + \sigma^2 \\
     \textbf{K}_k &= \textbf{P}_{k-1} \textbf{H}^T_k S_k^{-1} \\
     \textbf{m}_k &= \textbf{m}_{k-1} + \textbf{K}_k [y_k - \textbf{H}_k \textbf{m}_{k-1}] \\
     \textbf{P}_k &= \textbf{P}_{k-1} - \textbf{K}_k S_k \textbf{K}_k^T
\end{align*}
Here $\mathbf{K}$ is the Kalman gain that controls the weight we give to new
measurements.
This is actually a special case of The Kalman filter update equations.
Since here the parameters $\theta$ are assumed to stay constant there is
no stochastic dynamics model used for the prediction step.

For estimating the surrogate model for the MORE algorithm we have to
deal with dynamic parameters instead. In the next chapter we will therefore
introduce a drift model for the parameters.

\subsubsection{Drift model}
We assume the parameter $\theta$ performs a \textit{Gaussian Random Walk} after
each time step.
\begin{align}
  p(y_k | \theta) &= \text{N}(y_k | \mathbf{H}_k \theta_k, \sigma^2) \\
  p(\theta_k | \theta_{k-1}) &= \text{N} (\theta_k | \theta_{k-1}, Q) \\
  p(\theta_0) &= \text{N}(\theta_0 | \mathbf{m}_0, \mathbf{P}_0)
\end{align}

Where $\mathbf{Q}$ is the covariance of the random walk. 
Starting with the distribution

$$ p(\theta_{k-1} | y_{1:k-1})
= \text{N}(\theta_{k-1} | \mathbf{m}_{k-1}, \mathbf{P}_{k-1}) $$

the joint distribution of $\theta_k$ and $\theta_{k-1}$ is

$$ p(\theta_k, \theta_{k-1} | y_{1:k-1})
= p(\theta_k | \theta{k-1}) | y_{1:k-1})
$$

The distribution of $\theta_k$ given the measurement history up to time
step $k - 1$ can be calculated by integrating over
$\theta_{k-1}$:
$$ p(\theta_k | y_{1:k-1})
= \int p(\theta_k | \theta_{k-1}) p(\theta_{k-1} | y_{1:k-1}) d\theta_{k-1}
$$

This relationship is sometimes called \textit{Chapman-Kolmogorov equation}
The result of the marginalization is Gaussian

$$ p(\theta_k | y_{1:k-1}) = \text{N}(\theta_k | m_k^{-}, P_k^{-}) $$

with
\begin{align}
  m_k^{-} &= m_{k-1} \\
  P_k^{-} &= P_{k-1} + Q
\end{align}

Now we simply replace $P_{k-1}$ with $P_k^{-}$ in our update equations:
\begin{align*}
     S_k &= \textbf{H}_k \textbf{P}_{k}^{-} \textbf{H}^T_k + \sigma^2 \\
     \textbf{K}_k &= \textbf{P}_{k}^{-} \textbf{H}^T_k S_k^{-1} \\
     \textbf{m}_k &= \textbf{m}_{k}^{-} + \textbf{K}_k [y_k - \textbf{H}_k \textbf{m}_{k}^{-}] \\
     \textbf{P}_k &= \textbf{P}_{k}^{-} - \textbf{K}_k S_k \textbf{K}_k^T
\end{align*}


These equations are a special case of the Kalman Filter, only doing
a adding noise to the parameters in the prediction step and using
an identity matrix as the state transition model. There is no
obvious state transition model for the general case, maybe some
method based on an momentum approach on the differences in subsequent
estimated parameters. Otherwise this could be engineered to a certain
objective function, but this is part of future work.