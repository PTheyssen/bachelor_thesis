% !TeX spellcheck = de_DE
% !TeX encoding = UTF-8
% !TeX root = ../thesis.tex

\chapter{Appendix}

\section{MORE: Equations}\label{more_appendix}
Forming the Lagrangian for the constraint optimization problem we get
\begin{align*} \mathfrak{L}(\pi, \eta, \omega) = 
\int \pi(\theta) \mathcal{R}_{\theta} d\theta \; + \; 
\eta  \left(\epsilon - \int \pi(\theta) \text{ log}
 \frac{\pi(\theta)}{q(\theta)} d\theta\right)
 - \; \omega \left(\beta + \int \pi(\theta) \text{ log}(\pi(\theta)) d\theta\right)
\end{align*}

Optimizing the Lagrangian by computing the derivative with respect
to the mean and covariance matrix yields
\begin{align*}
\pi(\theta) \propto q(\theta)^{\eta/(\eta+\omega)} 
\text{exp}\left(\frac{\mathcal{R}_\theta}{\eta + \omega}\right)
\end{align*}
the solution depends on the quadratic and linear term
of the surrogate model and the
Lagrangian multipliers
$\eta$ and $\omega$. These in turn can be obtained by
minimizing the dual function:
\begin{align}
  \label{eq:dual}
  g(\eta,\omega) = \eta\epsilon - \omega\beta + (\eta - \omega) \text{log}
\left(\int q(\theta)^{\frac{\eta}{\eta + \omega}}
  \text{exp}\left(\frac{\mathcal{R}_\theta}{\eta + \omega}\right) d\theta \right)
\end{align}

Assuming we are given a quadratic surrogate model
$$ \mathcal{R}_\theta \approx \theta^T \mathbf{R} \theta + \theta^T \mathbf{r} + r_0 $$
we can solve the dual function in closed form.
$$ g(\eta, \omega) = \eta \epsilon - \beta \omega
+ \frac{1}{2} \left(\mathbf{f}^T \mathbf{F} \mathbf{f}
  - \eta \mathbf{b}^T \mathbf{Q}^{-1}
  \mathbf{b} - \eta \log |2\pi \mathbf{Q}|p
  + (\eta + \omega) \log |2\pi (\eta + \omega)
\mathbf{F}| \right) $$
with $\mathbf{F} = (\eta \Sigma^{-1} - 2 \mathbf{R})^{-1}$ and
$\mathbf{f} = \eta \Sigma^{-1} \mu + \mathbf{r}$


\section{Gaussian probability function}\label{gauss_pdf}
A random variable $x \in \mathbb{R}^n$ has a Gaussian distribution with mean
$m \in \mathbb{R}^n$ and covariance $P \in \mathbb{R}^{n\times n}$ if its
probability density has the form
$$ \mathcal{N}(x | m, P) = \frac{1}{(2\pi)^{n / 2} |P|^{1/2}}
\text{exp} \left( -\frac{1}{2} (x - m)^T P^{-1} (x-m) \right) $$
where $|P|$ is the determinant of the matrix $P$.

\section{Hyperparameter Search}
\label{appendix:par_search}
Here we first describe the different parameters
that are optimized
then present results and observations from
our hyperparameter search.

\subsubsection{MORE parameters}
\begin{itemize}
\item KL-Bound $\epsilon$
\item entropy-loss bound $\beta$
\end{itemize}
\subsubsection{RLS parameters}
\begin{itemize}
\item model noise $\mathbf{Q}$
\item measurement noise $\sigma^2$
\end{itemize}

\subsubsection{Configurations}
The best configurations we found for our tasks

- table Rosenbrock

- table reaching task

\begin{table}[htp]
  \centering
  \begin{tabular}{lccc} \toprule
    Task (1500 MORE Iterations)
    & Least Squares & RLS with Pool \\ \midrule
    Rosenbrock Function 15D & 16 & 30 \\ \bottomrule
  \end{tabular}
  \caption{
    Computational time for Rosenbrock function.
  }
  \label{tab:table}
\end{table}

\begin{table}[htp]
  \centering
  \begin{tabular}{lccc} \toprule
    Task (300 MORE Iterations)
    & Least Squares & RLS with Pool & RLS without Pool  \\ \midrule
    Via-point Reaching Task & 52 & 102 & 50 \\ \bottomrule
  \end{tabular}
  \caption{
    Computational time for via-point task.
  }
  \label{tab:table}
\end{table}

- configurations of best results list parameters used
for best performing algorithm used in plots

- include extensive hyperparameter search figures and tables

- table for runtimes

\subsection{Further Tests and Observations}
In \Cref{fig:noise_weighting} the influence of adding model noise is
illustrated

 \begin{figure}[ht!]
  \centering
     \input{figures/rls_model_noise}
     \hspace{1cm}                       
     \caption{
       We see LS approach, RLS approach with and without a pool.
       On the right we see the resulting movement of the RLS with pool.
     }
     \label{fig:noise_weighting}  
\end{figure}

- influence of KL bound on algorithm

- influence entropy loss and initial entropy, variance

- test whitening and normalization


